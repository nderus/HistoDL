{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow # cv2.imshow does not work on Google Colab notebooks, which is why we are using cv2_imshow instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "print(x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "  print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.VGG19(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(50, 50, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        #self.beta_coefficient = beta_coefficient\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)[2]\n",
    "        return self.decoder(x)\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_sum(\n",
    "                    keras.losses.MSE(data, reconstruction), axis=(1, 2) # mod\n",
    "                )\n",
    "           # )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "vae_encoder = load_model('models/vae_encoder.h5')\n",
    "vae_decoder = load_model('models/vae_decoder.h5')\n",
    "vae = VAE(encoder=vae_encoder, decoder=vae_decoder, beta = 1)\n",
    "vae(np.zeros((1,50,50,3)))\n",
    "vae.compile(optimizer='Adam')\n",
    "vae.load_weights('weights/vae.h5')\n",
    "model = vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preprocess the image into a 4D tensor\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('datasets/breast-histopathology/IDC_regular_ps50_idx5/13689/1/13689_idx5_x801_y1501_class1.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "image = cv2.resize(image, (50, 50))\n",
    "image = image.astype('float32') / 255\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "print(image.shape)\n",
    "image.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model \n",
    "        self.model_input = model.encoder\n",
    "        self.model_output = model.decoder\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.decoder.layers):#added decoder\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = Model(\n",
    "            inputs=[self.model_input.inputs],\n",
    "            outputs=[self.model_output.get_layer(self.layerName).output, self.model_output.output])\n",
    "\n",
    "        encoder_gradModel = Model(inputs=[self.model_input.inputs],\n",
    "                                   outputs=[self.model_input.output])\n",
    "        decoder_gradModel = Model(inputs =[encoder_gradModel.output], \n",
    "                                 outputs = [self.model_output.ouput])\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = decoder_gradModel(self.model_input(inputs))\n",
    "            \n",
    "            loss = predictions[:, tf.argmax(predictions[0])]\n",
    "    \n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h)) #\n",
    "        #heatmap = tf.image.resize(cam , (w, h))\n",
    "        \n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "\n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "                        colormap=cv2.COLORMAP_VIRIDIS):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.encoder.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(image) \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.argmax(preds[0])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model.decoder.layers)):\n",
    "  print(model.decoder.get_layer(index = idx).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradCAM at 0x7fee1c3b1040>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "icam = GradCAM(model, i, 'conv2d_transpose_2') #last layer\n",
    "icam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name='z_sampling'), name='z_sampling', description=\"created by layer 'z_sampling'\") at layer \"dense_4\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000018vscode-remote?line=0'>1</a>\u001b[0m heatmap \u001b[39m=\u001b[39m icam\u001b[39m.\u001b[39;49mcompute_heatmap(image)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000018vscode-remote?line=1'>2</a>\u001b[0m heatmap\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb Cell 12'\u001b[0m in \u001b[0;36mGradCAM.compute_heatmap\u001b[0;34m(self, image, eps)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_heatmap\u001b[39m(\u001b[39mself\u001b[39m, image, eps\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=33'>34</a>\u001b[0m     \u001b[39m# construct our gradient model by supplying (1) the inputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=34'>35</a>\u001b[0m     \u001b[39m# to our pre-trained model, (2) the output of the (presumably)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=35'>36</a>\u001b[0m     \u001b[39m# final 4D layer in the network, and (3) the output of the\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=36'>37</a>\u001b[0m     \u001b[39m# softmax activations from the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=37'>38</a>\u001b[0m     gradModel \u001b[39m=\u001b[39m Model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=38'>39</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_input\u001b[39m.\u001b[39;49minputs],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=39'>40</a>\u001b[0m         outputs\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_output\u001b[39m.\u001b[39;49mget_layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayerName)\u001b[39m.\u001b[39;49moutput, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_output\u001b[39m.\u001b[39;49moutput])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=41'>42</a>\u001b[0m     encoder_gradModel \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input\u001b[39m.\u001b[39minputs],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=42'>43</a>\u001b[0m                                outputs\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input\u001b[39m.\u001b[39moutput])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=43'>44</a>\u001b[0m     decoder_gradModel \u001b[39m=\u001b[39m Model(inputs \u001b[39m=\u001b[39m[encoder_gradModel\u001b[39m.\u001b[39moutput], \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/visualizing_heatmap_vae.ipynb#ch0000011vscode-remote?line=44'>45</a>\u001b[0m                              outputs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output\u001b[39m.\u001b[39mouput])\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=527'>528</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=528'>529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=529'>530</a>\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=530'>531</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=531'>532</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py:146\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=142'>143</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m([functional_utils\u001b[39m.\u001b[39mis_input_keras_tensor(t)\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=143'>144</a>\u001b[0m               \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)]):\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=144'>145</a>\u001b[0m     inputs, outputs \u001b[39m=\u001b[39m functional_utils\u001b[39m.\u001b[39mclone_graph_nodes(inputs, outputs)\n\u001b[0;32m--> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=145'>146</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_graph_network(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=527'>528</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=528'>529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=529'>530</a>\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=530'>531</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py?line=531'>532</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py:229\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=225'>226</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_coordinates\u001b[39m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=227'>228</a>\u001b[0m \u001b[39m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=228'>229</a>\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[39m=\u001b[39m _map_graph_network(\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=229'>230</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs)\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=230'>231</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_nodes \u001b[39m=\u001b[39m nodes\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=231'>232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes_by_depth \u001b[39m=\u001b[39m nodes_by_depth\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py:1036\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1033'>1034</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(node\u001b[39m.\u001b[39mkeras_inputs):\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1034'>1035</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(x) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m computable_tensors:\n\u001b[0;32m-> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1035'>1036</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1036'>1037</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGraph disconnected: cannot obtain value for tensor \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1037'>1038</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mat layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. The following previous layers \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1038'>1039</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwere accessed without issue: \u001b[39m\u001b[39m{\u001b[39;00mlayers_with_complete_input\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1039'>1040</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(node\u001b[39m.\u001b[39moutputs):\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/functional.py?line=1040'>1041</a>\u001b[0m   computable_tensors\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(x))\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name='z_sampling'), name='z_sampling', description=\"created by layer 'z_sampling'\") at layer \"dense_4\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "heatmap = icam.compute_heatmap(image)\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icam = GradCAM(model, i, 'conv_block_2') #last convolutional layer\n",
    "heatmap = icam.compute_heatmap(image)\n",
    "heatmap = cv2.resize(heatmap, (50, 50))\n",
    "\n",
    "image = cv2.imread('datasets/breast-histopathology/IDC_regular_ps50_idx5/13689/1/13689_idx5_x801_y1501_class1.png')\n",
    "image = cv2.resize(image, (50, 50))\n",
    "\n",
    "print(heatmap.shape, image.shape)\n",
    "print(heatmap.dtype)\n",
    "\n",
    "(heatmap, output) = icam.overlay_heatmap(heatmap, image.squeeze(), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "ax[0].imshow(heatmap)\n",
    "ax[1].imshow(image.squeeze())\n",
    "ax[2].imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heatmap():\n",
    "    image = cv2.imread('datasets/breast-histopathology/IDC_regular_ps50_idx5/13689/1/13689_idx5_x801_y1501_class1.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "    image = cv2.resize(image, (50, 50))\n",
    "    image = image.astype('float32') / 255\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    icam = GradCAM(model, i, 'conv_block_2') #last convolutional layer\n",
    "    heatmap = icam.compute_heatmap(image)\n",
    "    heatmap = cv2.resize(heatmap, (50, 50))\n",
    "\n",
    "    image = cv2.imread('datasets/breast-histopathology/IDC_regular_ps50_idx5/13689/1/13689_idx5_x801_y1501_class1.png')\n",
    "    image = cv2.resize(image, (50, 50))\n",
    "\n",
    "    print(heatmap.shape, image.shape)\n",
    "    print(heatmap.dtype)\n",
    "\n",
    "    (heatmap, output) = icam.overlay_heatmap(heatmap, image.squeeze(), alpha=0.3)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "    ax[0].imshow(heatmap)\n",
    "    ax[1].imshow(image.squeeze())\n",
    "    ax[2].imshow(output)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "image_count = 10\n",
    "\n",
    "_, axs = plt.subplots(image_count, 3, figsize=(20, 20))\n",
    "for i in range(image_count):\n",
    "  random_idx=random.randint(0, train_x.shape[0])\n",
    "  axs[i].compute_heatmap()\n",
    "  axs[i].axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "dis_vae",
   "language": "python",
   "name": "dis_vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
