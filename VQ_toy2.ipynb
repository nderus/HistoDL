{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return np.array([i for i, elem in enumerate(lst) if condition(elem)])\n",
    "    \n",
    "def plot_2d_data_categorical(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  _, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "  colors = np.array(['#7FFFD4', '#458B74', '#0000CD', '#EE3B3B', '#7AC5CD', '#66CD00',\n",
    "         '#EE7621', '#3D59AB', '#CD950C', '#483D8B'])\n",
    "  for i in range(len(data_2d)):\n",
    "      for k in range(category_count):\n",
    "\n",
    "        index = find_indices(y[i], lambda e: e == k)\n",
    "\n",
    "        data_2d_k = data_2d[i][index, ]\n",
    "        y_k = y[i][index]\n",
    "\n",
    "        if (titles != None):\n",
    "          axs[k,i].set_title(titles[i])\n",
    "\n",
    "        scatter = axs[k, i].scatter(data_2d_k[:, 0], data_2d_k[:, 1],\n",
    "                                s=1, c=colors[k], cmap=plt.cm.Paired)\n",
    "        axs[k, i].legend(*scatter.legend_elements())\n",
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "\n",
    "def plot_2d_data_category(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  _, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count=10 \n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.expand_dims(train_x, -1)\n",
    "test_x = np.expand_dims(test_x, -1)\n",
    "train_x_scaled = (train_x / 255.0) - 0.5\n",
    "test_x_scaled = (test_x / 255.0) - 0.5\n",
    "\n",
    "data_variance = np.var(train_x / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x_scaled = (train_x / 255.0) - 0.5\n",
    "test_x_scaled = (test_x / 255.0) - 0.5\n",
    "\n",
    "data_variance = np.var(train_x / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu brings a lot of activation values = 0, leaky seems better\n",
    "# https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24\n",
    "\n",
    "def bn_relu(inputs):\n",
    "    bn = layers.BatchNormalization()(inputs)\n",
    "    relu = layers.LeakyReLU(0.2)(bn)\n",
    "    return(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=get_encoder().output.shape[1:])\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        latent_inputs\n",
    "    )\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = (\n",
    "            beta  # This parameter is best kept between [0.25, 2] as per the paper.\n",
    "        )\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = self.beta * tf.reduce_mean(\n",
    "            (tf.stop_gradient(quantized) - x) ** 2\n",
    "        )\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vqvae(latent_dim=16, num_embeddings=64):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    inputs = keras.Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super(VQVAETrainer, self).__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 30\n",
    "batch_size = 128\n",
    "patience = 10\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=16, num_embeddings=128)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "vqvae_trainer.fit(train_x_scaled, epochs=epoch_count, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_subplot(original, reconstructed):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original.squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed.squeeze() + 0.5)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "trained_vqvae_model = vqvae_trainer.vqvae\n",
    "idx = np.random.choice(len(test_x_scaled), 10)\n",
    "test_images = test_x_scaled[idx]\n",
    "reconstructions_test = trained_vqvae_model.predict(test_images)\n",
    "\n",
    "for test_image, reconstructed_image in zip(test_images, reconstructions_test):\n",
    "    show_subplot(test_image, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vqvae_trainer.vqvae.get_layer(\"encoder\")\n",
    "quantizer = vqvae_trainer.vqvae.get_layer(\"vector_quantizer\")\n",
    "\n",
    "encoded_outputs = encoder.predict(test_images)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_images[i].squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(codebook_indices[i])\n",
    "    plt.title(\"Code\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_residual_blocks = 2\n",
    "num_pixelcnn_layers = 2\n",
    "pixelcnn_input_shape = encoded_outputs.shape[1:-1]\n",
    "print(f\"Input shape of the PixelCNN: {pixelcnn_input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class PixelConvLayer(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super(PixelConvLayer, self).__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "# Next, we build our residual block layer.\n",
    "# This is just a normal residual block, but based on the PixelConvLayer.\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = PixelConvLayer(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return keras.layers.add([inputs, x])\n",
    "\n",
    "\n",
    "pixelcnn_inputs = keras.Input(shape=pixelcnn_input_shape, dtype=tf.int32)\n",
    "ohe = tf.one_hot(pixelcnn_inputs, vqvae_trainer.num_embeddings)\n",
    "x = PixelConvLayer(\n",
    "    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n",
    ")(ohe)\n",
    "\n",
    "for _ in range(num_residual_blocks):\n",
    "    x = ResidualBlock(filters=128)(x)\n",
    "\n",
    "for _ in range(num_pixelcnn_layers):\n",
    "    x = PixelConvLayer(\n",
    "        mask_type=\"B\",\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = keras.layers.Conv2D(\n",
    "    filters=vqvae_trainer.num_embeddings, kernel_size=1, strides=1, padding=\"valid\"\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = keras.Model(pixelcnn_inputs, out, name=\"pixel_cnn\")\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the codebook indices.\n",
    "encoded_outputs = encoder.predict(train_x_scaled)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "\n",
    "codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "print(f\"Shape of the training data for PixelCNN: {codebook_indices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "pixel_cnn.fit(\n",
    "    x=codebook_indices,\n",
    "    y=codebook_indices,\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Codebook sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mini sampler model.\n",
    "inputs = layers.Input(shape=pixel_cnn.input_shape[1:])\n",
    "x = pixel_cnn(inputs, training=False)\n",
    "dist = tfp.distributions.Categorical(logits=x)\n",
    "sampled = dist.sample()\n",
    "sampler = keras.Model(inputs, sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array of priors.\n",
    "batch = 10\n",
    "priors = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols = priors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over the priors because generation has to be done sequentially pixel by pixel.\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "        # pixel.\n",
    "        probs = sampler.predict(priors)\n",
    "        # Use the probabilities to pick pixel values and append the values to the priors.\n",
    "        priors[:, row, col] = probs[:, row, col]\n",
    "\n",
    "print(f\"Prior shape: {priors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an embedding lookup.\n",
    "pretrained_embeddings = quantizer.embeddings\n",
    "priors_ohe = tf.one_hot(priors.astype(\"int32\"), vqvae_trainer.num_embeddings).numpy()\n",
    "quantized = tf.matmul(\n",
    "    priors_ohe.astype(\"float32\"), pretrained_embeddings, transpose_b=True\n",
    ")\n",
    "quantized = tf.reshape(quantized, (-1, *(encoded_outputs.shape[1:])))\n",
    "\n",
    "# Generate novel images.\n",
    "decoder = vqvae_trainer.vqvae.get_layer(\"decoder\")\n",
    "generated_samples = decoder.predict(quantized)\n",
    "\n",
    "for i in range(batch):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(priors[i])\n",
    "    plt.title(\"Code\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(generated_samples[i].squeeze() + 0.5)\n",
    "    plt.title(\"Generated Sample\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_size = 10\n",
    "_, input_label_train, train_input = cvae.conditional_input([train_x, train_y_one_hot])\n",
    "_, input_label_test, test_input = cvae.conditional_input([test_x, test_y_one_hot])\n",
    "_, input_label_val, val_input = cvae.conditional_input([val_x, val_y_one_hot])\n",
    "\n",
    "\n",
    "print(input_label_train.shape)\n",
    "print(train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict(train_input)\n",
    "test_x_mean, test_log_var = cvae.encoder.predict(test_input)\n",
    "val_x_mean, val_log_var = cvae.encoder.predict(val_input)\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim > 2:\n",
    "    from sklearn import manifold\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    train_x_tsne = tsne.fit_transform(train_x_mean[:2000])\n",
    "    test_x_tsne = tsne.fit_transform(test_x_mean[:2000])\n",
    "    val_x_tsne = tsne.fit_transform(val_x_mean[:2000])\n",
    "    plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:2000], test_y[:2000] ,val_y[:2000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "            [train_y, test_y ,val_y],\n",
    "            ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "    plot_2d_data( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, input_label_train)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, input_label_test)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, input_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 5\n",
    "\n",
    "_, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  axs[0, i].set_title(train_y[random_idx])\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim), mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 5\n",
    "digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[0].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 5\n",
    "_, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "    digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "    a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "    b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "    z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    digit_0 = decoded_x[0].reshape(input_shape) \n",
    "    digit_1 = decoded_x[1].reshape(input_shape) \n",
    "    axs[0, i].imshow(digit_0)\n",
    "    axs[0, i].axis('off')\n",
    "    axs[0, i].set_title(digit_label)\n",
    "    axs[1, i].imshow(digit_1)\n",
    "    axs[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.save_weights('weights/cvae_toy.h5')\n",
    "cvae_encoder.save('models/cvae_encoder_toy.h5')\n",
    "cvae_decoder.save('models/cvae_decoder_toy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "  n = 10  # number of images per row and column\n",
    "  limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "  first_dim_const= 0  # constant value of the second latent dimension\n",
    "\n",
    "  grid_y = np.linspace(-limit,limit, n) \n",
    "\n",
    "  generated_images=[]\n",
    "  for digit_label in range(category_count):\n",
    "    digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "    \n",
    "    single_row_generated_images=[]\n",
    "    for i, yi in enumerate(grid_y):\n",
    "      random_sample = np.array([[first_dim_const, yi]])\n",
    "      z_cond = sampling(z_mean=random_sample, z_log_var=0.3,\n",
    "                      input_label = digit_label_one_hot )\n",
    "      decoded_x = cvae_decoder.predict(z_cond)\n",
    "      single_row_generated_images.append(decoded_x[0].reshape(input_shape))\n",
    "    generated_images.append(single_row_generated_images)      \n",
    "\n",
    "  plot_generated_images(generated_images,n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvae.built = True\n",
    "#cvae.load_weights('weights/vae_toy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = cvae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "test = test_x[1]\n",
    "plt.imshow(test)\n",
    "test = image.img_to_array(test)\n",
    "test = np.expand_dims(test, axis=0)\n",
    "test.shape\n",
    "test_label = test_y_one_hot[0]\n",
    "img_tensor = [test, test_label]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "import tensorflow as tf\n",
    "\n",
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers[1:]:\n",
    "    \n",
    "    try: \n",
    "        layer_outputs.append(layer.get_output_at(1))\n",
    "        layer_names.append(layer.name)\n",
    "    \n",
    "    except:\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a list of 5 Numpy arrays:\n",
    "# one array per layer activation\n",
    "if 'encoder' in model.name:\n",
    "    input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "    activations = activation_model.predict(conditional_input) #for encoder\n",
    "\n",
    "if 'decoder' in model.name:\n",
    "    input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "    input_label = np.expand_dims(input_label, axis=0)\n",
    "    z_mean, z_log_var = cvae.encoder(conditional_input)\n",
    "    z_cond = cvae.sampling(z_mean, z_log_var, input_label)\n",
    "    \n",
    "    activations = activation_model.predict(z_cond) #for decoder\n",
    "\n",
    "len(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_filters(activation_layer, layer_name, counter):\n",
    "    if len(activation_layer.shape) == 2: # if flat layer\n",
    "        print('flat')\n",
    "        return None\n",
    "        if activation_layer.shape[1] == 1875:\n",
    "            activation_layer = activation_layer.reshape(1, 25, 25, 3)\n",
    "        if activation_layer.shape[1] == 1024:\n",
    "           activation_layer = activation_layer.reshape(1, 16, 16, 4)\n",
    "        if activation_layer.shape[1] == 512:\n",
    "           activation_layer = activation_layer.reshape(1, 8, 8, 8)\n",
    "\n",
    "    n = math.floor(np.sqrt(activation_layer.shape[3]))\n",
    "\n",
    "    if int(n + 0.5) ** 2 == activation_layer.shape[3]:\n",
    "\n",
    "        m = n\n",
    "    else:\n",
    "        m = math.floor(activation_layer.shape[3] / n)\n",
    "\n",
    "    if activation_layer.shape[3] == 1:\n",
    "        fig, ax = plt.subplots(1, 1, sharex='col', sharey='row',figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "        ax.imshow(activation_layer[0,:, :, 0], cmap='viridis')\n",
    "        fig.savefig('img/activations/vae/{}_{}_activations_{}.png'.format(model.name, counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "            \n",
    "    if n == 1:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, sharex='col', sharey='row',figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(activation_layer[0,:, :, i], cmap='viridis')\n",
    "        fig.savefig('img/activations/vae/{}_{}_activations_{}.png'.format(model.name, counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "    fig, ax = plt.subplots(n, m, sharex='col', sharey='row',figsize=(15, 15))\n",
    "    fig.suptitle(layer_name)\n",
    "    \n",
    " \n",
    "\n",
    "    filter_counter = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(activation_layer[0,:, :, filter_counter], cmap='viridis')\n",
    "            filter_counter += 1\n",
    "            if filter_counter == (activation_layer.shape[3] ):\n",
    "                break\n",
    "    \n",
    "    fig.savefig('img/activations/vae/{}_{}_toy_activations_{}.png'.format(model.name, counter, layer_name))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation, name in zip(activations[0:], layer_names[0:]):\n",
    "\n",
    "    print(name)\n",
    "    print(activation.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter, (activation, name) in enumerate(zip(activations[0:], layer_names[0:])):\n",
    "    print(name)\n",
    "    plot_filters(activation, name, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conditional_input( inputs, label_size=10): \n",
    "  \n",
    "        image_size = [input_shape[0], input_shape[1], input_shape[2]]\n",
    "\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                        dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nth_filter_loss(filter_index, layer_name):\n",
    "    \"\"\"\n",
    "    We build a loss function that maximizes the activation\n",
    "    of the nth filter of the layer considered\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # Initiate random noise\n",
    "    # Create a connection between the input and the target layer\n",
    "    \n",
    "    submodel = tf.keras.models.Model([model.inputs[0]], [model.get_layer(layer_name).output])\n",
    "\n",
    "# Initiate random noise\n",
    "\n",
    "    input_img_data = np.random.random((1, input_shape[0], input_shape[1], 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128.\n",
    "\n",
    "    # Cast random noise from np.float64 to tf.float32 Variable\n",
    "    input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "    data = [input_img_data, train_y_one_hot[0]]\n",
    "    _, _, conditional_input_img = filter_conditional_input(data)\n",
    "    conditional_input_img= tf.Variable(tf.cast(conditional_input_img, tf.float32))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = submodel(conditional_input_img)\n",
    "            loss_value = tf.reduce_mean(outputs[:, :, :, filter_index])\n",
    "        grads = tape.gradient(loss_value, conditional_input_img)\n",
    "        normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "        conditional_input_img.assign_add(normalized_grads * step_size)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    #iterate = K.function([input_img], [loss_value, grads])\n",
    "\n",
    "#if loss_value > 0:\n",
    "    img = conditional_input_img.numpy().astype(np.float64)\n",
    "    img = img.squeeze()\n",
    "    img = deprocess_image(img)\n",
    "    kept_filters.append((img, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Layer name to inspect\n",
    "layer_name = 'block1_conv1'\n",
    "\n",
    "epochs = 100\n",
    "step_size = 1.\n",
    "filter_index = 1\n",
    "\n",
    "# Create a connection between the input and the target layer\n",
    "submodel = tf.keras.models.Model([model.inputs[0]], [model.get_layer(layer_name).output])\n",
    "submodel.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initiate random noise\n",
    "if 'encoder' in model.name:\n",
    "    input_img_data = np.random.random((1, 32, 32, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128.\n",
    "    # Cast random noise from np.float64 to tf.float32 Variable\n",
    "    input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "    data = [input_img_data, train_y_one_hot[0]]\n",
    "    _, _, conditional_input_img = filter_conditional_input(data)\n",
    "\n",
    "# Cast random noise from np.float64 to tf.float32 Variable\n",
    "conditional_input_img= tf.Variable(tf.cast(conditional_input_img, tf.float32))\n",
    "# Iterate gradient ascents\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = submodel(conditional_input_img)\n",
    "        loss_value = tf.reduce_mean(outputs[:, :, :, filter_index])   \n",
    "    grads = tape.gradient(loss_value, conditional_input_img)\n",
    "    normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "    conditional_input_img.assign_add(normalized_grads * step_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = conditional_input_img.numpy().astype(np.uint8)\n",
    "print(img.shape)\n",
    "img = img[:, :, :, :3]\n",
    "print(img.shape)\n",
    "img = img.squeeze()\n",
    "img = img / 255\n",
    "print(img.max())\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the generated pictures for each filter.\n",
    "img_width = input_shape[0]\n",
    "img_height = input_shape[1]\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "print(input_img.shape)\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "#layer_dict = dict([(layer.name, layer) for layer in model.layers[0:]])\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_filters = [layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 100\n",
    "step_size = 1.\n",
    "kept_filters = []\n",
    "filters_dict = dict()\n",
    "for layer_name in layers_filters:\n",
    "    if 'conv' in layer_name:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        print('Processing filter for layer:', layer_name)\n",
    "        for filter_index in range(min(layer.output.shape[-1], 100)):\n",
    "            # print('Processing filter %d' % filter_index)\n",
    "\n",
    "            start_time = time.time()\n",
    "            build_nth_filter_loss(filter_index, layer_name)\n",
    "            end_time = time.time()\n",
    "            #print('--->Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "        filters_dict[layer.name] = kept_filters\n",
    "        kept_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_filters[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "def stich_filters(kept_filters, layer_name):\n",
    "    # By default, we will stich the best 64 (n*n) filters on a 8 x 8 grid.\n",
    "    n = int(np.sqrt(len(kept_filters)))\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top 64 filters.\n",
    "    kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    kept_filters = kept_filters[:n * n]\n",
    "\n",
    "    # build a black picture with enough space for\n",
    "    # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "    margin = 5\n",
    "    width = n * img_width + (n - 1) * margin\n",
    "    height = n * img_height + (n - 1) * margin\n",
    "    stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "    # fill the picture with our saved filters\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, loss = kept_filters[i * n + j]\n",
    "            width_margin = (img_width + margin) * i\n",
    "            height_margin = (img_height + margin) * j\n",
    "            stitched_filters[\n",
    "                width_margin: width_margin + img_width,\n",
    "                height_margin: height_margin + img_height, :] = img[:, :, :3]\n",
    "\n",
    "    # save the result to disk\n",
    "    save_img('img/filters/vae/{}_toy_stitched_filters_{}.png'.format(model.name, layer_name), stitched_filters)\n",
    "    \n",
    "for layer_name, kept_filters in filters_dict.items():\n",
    "    print('Stiching filters for {}'.format(layer_name))\n",
    "    stich_filters(kept_filters, layer_name)\n",
    "    print('number of filters kept:', len(kept_filters))\n",
    "    print('Completed.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
