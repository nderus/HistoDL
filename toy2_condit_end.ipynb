{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return np.array([i for i, elem in enumerate(lst) if condition(elem)])\n",
    "    \n",
    "def plot_2d_data_categorical(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  fig, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "  colors = np.array(['#7FFFD4', '#458B74', '#0000CD', '#EE3B3B', '#7AC5CD', '#66CD00',\n",
    "         '#EE7621', '#3D59AB', '#CD950C', '#483D8B'])\n",
    "  for i in range(len(data_2d)):\n",
    "      for k in range(category_count):\n",
    "\n",
    "        index = find_indices(y[i], lambda e: e == k)\n",
    "\n",
    "        data_2d_k = data_2d[i][index, ]\n",
    "        y_k = y[i][index]\n",
    "\n",
    "        if (titles != None):\n",
    "          axs[k,i].set_title(\"{} - Class: {}\".format(titles[i], k))\n",
    "\n",
    "        scatter = axs[k, i].scatter(data_2d_k[:, 0], data_2d_k[:, 1],\n",
    "                                s=1, c=colors[k], cmap=plt.cm.Paired)\n",
    "        axs[k, i].legend(*scatter.legend_elements())\n",
    "        axs[k, i].set_xlim([-3, 3])\n",
    "        axs[k, i].set_ylim([-3, 3])\n",
    "        wandb.log({\"Embdedding_classes\": wandb.Image(plt)})\n",
    "\n",
    "        \n",
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "    wandb.log({\"Embdedding\": wandb.Image(plt)})\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols, digit_label,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "  \n",
    "  wandb.log({\"Latent_interpolation_class: {}\".format(digit_label): wandb.Image(plt)})\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10):\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "    labels = ones * labels\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Val_Plot(loss, val_loss, reconstruction_loss, val_reconstruction_loss, kl_loss, val_kl_loss):\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize= (16,4))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(loss) + 1), loss)\n",
    "    ax1.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax1.set_title('History of Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "    ax2.plot(range(1, len(reconstruction_loss) + 1), reconstruction_loss)\n",
    "    ax2.plot(range(1, len(val_reconstruction_loss) + 1), val_reconstruction_loss)\n",
    "    ax2.set_title('History of reconstruction_loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('reconstruction_loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "    \n",
    "    ax3.plot(range(1, len(kl_loss) + 1), kl_loss)\n",
    "    ax3.plot(range(1, len(val_kl_loss) + 1), val_kl_loss)\n",
    "    ax3.set_title(' History of kl_loss')\n",
    "    ax3.set_xlabel(' Epochs ')\n",
    "    ax3.set_ylabel('kl_loss')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "    wandb.log({\"Training\": wandb.Image(plt)})\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"fashion_mnist\"\n",
    "category_count=10 \n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "         \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size=10000\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y,\n",
    "                            test_size = val_size,random_state = 1,shuffle=True)\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Validation data flatten shape: ',val_x.shape)\n",
    "print('Validation label shape: ',val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_x.shape) == 3:\n",
    "    train_x=np.expand_dims(train_x,axis=3)\n",
    "    val_x=np.expand_dims(val_x,axis=3)\n",
    "    test_x=np.expand_dims(test_x,axis=3)\n",
    "    print('Train shape: ',train_x.shape)\n",
    "    print('Validation shape: ',val_x.shape)\n",
    "    print('Test shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.0\n",
    "val_x = val_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_shape=(train_x.shape[1], train_x.shape[2])\n",
    "\n",
    "train_x_flatten=np.reshape(train_x,(train_x.shape[0],-1))\n",
    "val_x_flatten=np.reshape(val_x,(val_x.shape[0],-1))\n",
    "test_x_flatten=np.reshape(test_x,(test_x.shape[0],-1))\n",
    "\n",
    "print('Train data flatten shape: ',train_x_flatten.shape)\n",
    "print('Validation data flatten shape: ',val_x_flatten.shape)\n",
    "print('Test data flatten shape: ',test_x_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = to_categorical(train_y,category_count)\n",
    "val_y_one_hot=to_categorical(val_y,category_count)\n",
    "test_y_one_hot=to_categorical(test_y,category_count)\n",
    "\n",
    "print('Train label one hot encoding shape: ',train_y_one_hot.shape)\n",
    "print('Validation label one hot encoding shape: ',val_y_one_hot.shape)\n",
    "print('Test label one hot encoding shape: ',test_y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu brings a lot of activation values = 0, leaky seems better\n",
    "# https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24\n",
    "\n",
    "def bn_relu(inputs):\n",
    "    bn = layers.BatchNormalization()(inputs)\n",
    "    relu = layers.LeakyReLU(0.2)(bn)\n",
    "    return(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters: int, kernel_size: int = 3):\n",
    "    y = layers.Conv2D(kernel_size=kernel_size,\n",
    "               strides= 1,\n",
    "               filters=filters,\n",
    "               padding=\"same\")(x)\n",
    "    y = bn_relu(y)\n",
    "    y = layers.Conv2D(kernel_size=kernel_size,\n",
    "               strides=1,\n",
    "               filters=filters,\n",
    "               padding=\"same\")(y)\n",
    "\n",
    "    out = layers.Concatenate()([x, y])\n",
    "    out = bn_relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder3( input_shape = (28, 28, 1),  label_size=10, encoded_dim = 2): \n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape[0],\n",
    "                input_shape[1], input_shape[2]), dtype='float32',\n",
    "                name='Input')\n",
    "    input_label = layers.Input(shape=(label_size ),\n",
    "                                dtype ='float32')\n",
    "    \n",
    "    #inputs = layers.Input(shape = input_shape)\n",
    "    #labels_inputs = layers.Input(shape = (50, 50, 2))\n",
    "    #encoder_inputs = layers.Concatenate()([inputs, labels_inputs])\n",
    "    #block 1\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(inputs)\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = bn_relu(x)\n",
    "    # block 2\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "\n",
    "    x = bn_relu(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2,name='S4')(x)\n",
    "\n",
    "    # block 3\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                padding='same',\n",
    "                name='block3_conv2')(x)    \n",
    "    x = bn_relu(x)            \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(encoded_dim * 2 )(x)\n",
    "    f = layers.Concatenate()([y, input_label])\n",
    "    mu = layers.Dense(encoded_dim, name='mu')(f)\n",
    "    log_var = layers.Dense(encoded_dim, name='log_var')(f)\n",
    "    \n",
    "\n",
    "    model = keras.Model([inputs, input_label], [mu, log_var], name='encoder')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder3(input_shape, encoded_dim = 2,label_size=10): \n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(encoded_dim + label_size,),\n",
    "                                 name='decoder_input')\n",
    "    x = layers.Dense(encoded_dim)\n",
    "\n",
    "    x = layers.Dense(encoded_dim * 2 )\n",
    " \n",
    "    x = layers.Dense(input_shape[0]/2 * input_shape[1]/2 *64)(decoder_inputs)\n",
    "   \n",
    "    x = layers.Reshape(target_shape=(int(input_shape[0]/2),\n",
    "                     int(input_shape[1]/2), 64))(x)\n",
    "    x = bn_relu(x) \n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block4_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                    padding='same',\n",
    "                    name='up_block4_conv2')(x)  \n",
    "    x = bn_relu(x) \n",
    "    # block 2\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv2')(x)\n",
    "    x = bn_relu(x) \n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block6_conv1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                    padding='same',\n",
    "                    name='up_block6_conv2')(x)\n",
    "    x = bn_relu(x)                                \n",
    "    outputs = layers.Conv2DTranspose(filters=input_shape[-1], kernel_size=2,\n",
    "                             strides=1, activation='sigmoid',padding='same')(x)\n",
    "\n",
    "    model = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder = encoder3(encoded_dim = encoded_dim, input_shape = input_shape)\n",
    "cvae_decoder = decoder3(encoded_dim = encoded_dim, input_shape = input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, shape, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.shape = shape\n",
    "        self.latent_var = []\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        #\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"val_reconstruction_loss\")\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.val_total_loss_tracker,\n",
    "            self.val_reconstruction_loss_tracker,\n",
    "            self.val_kl_loss_tracker,\n",
    "\n",
    "        ]\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "    \n",
    "    def conditional_input(self, inputs, label_size=10): \n",
    "  \n",
    "        image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    \n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                      dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "    def sampling(self, z_mean, z_log_var, input_label):\n",
    "        if len(input_label.shape) == 1:\n",
    "            input_label = np.expand_dims(input_label, axis=0)\n",
    "\n",
    "        eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                               mean=0., stddev=1.0, name='epsilon')\n",
    "        z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1)\n",
    "        return z_cond\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "           \n",
    "            input_img, input_label, conditional_input = self.conditional_input(data) # TO DO: modify/remove, only need the input_label var\n",
    "            z_mean, z_log_var = self.encoder(data) \n",
    "            z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "            #reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "            #                        tf.keras.backend.flatten(reconstruction))\n",
    "            reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img, # removed np.prod(self.shape) *\n",
    "                                    reconstruction), axis=(1, 2))            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                      - tf.exp(z_log_var))\n",
    "            #kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)) # was just reduce_sum\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1) #sum over encoded dimensiosn, average over batch\n",
    "            kl_loss = self.beta * kl_loss\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            total_loss = tf.reduce_mean(total_loss) #not necessary since I added red mean in kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        #wandb.log({\"loss\": total_loss, \"reconstructon_loss\": reconstruction_loss, \"kl_loss\": kl_loss,})\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var = self.encoder(data)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))   \n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                  - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        kl_loss = self.beta * kl_loss\n",
    "        total_loss =  reconstruction_loss + kl_loss\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return{\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl_coefficient = encoded_dim / (input_shape[0] * input_shape[1] * input_shape[2])\n",
    "kl_coefficient = 1\n",
    "print('kl coefficient: {:.3f}'.format(kl_coefficient))\n",
    "# from b vae paper, use beta = encoded_dimension / pixel_dimension i.e. -> 0.068\n",
    "cvae = CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)\n",
    "cvae.built = True\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_input = cvae.encoder.input[0]\n",
    "cvae_output = cvae.decoder.output\n",
    "mu = cvae.encoder.get_layer('mu').output\n",
    "log_var = cvae.encoder.get_layer('log_var').output\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "cvae.compile(optimizer = opt)\n",
    "#cvae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean = np.random.normal(size=(100, 512))\n",
    "z_log_var =  np.random.normal(size=(100, 512))\n",
    "\n",
    "kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "            - tf.exp(z_log_var))\n",
    "\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kl_loss = tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "            - tf.exp(z_log_var)), axis=1) #sum over encoded dimensions\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "#kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.random.normal(size=(100, 32, 32, 3))\n",
    "reconstruction = np.random.normal(size=(100, 32, 32, 3))\n",
    "\n",
    "reconstruction_loss = np.prod(input_img.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "                                    tf.keras.backend.flatten(reconstruction)) \n",
    "\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstruction_loss = tf.reduce_sum(\n",
    "\n",
    "                keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))\n",
    "\n",
    "reconstruction_loss.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = kl_coefficient * kl_loss\n",
    "total_loss =  reconstruction_loss + kl_loss\n",
    "total_loss = tf.reduce_mean(total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "#wandb.init(project=\"my-test-project\", entity=\"nrderus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=\"HistoDL\", entity=\"nrderus\",\n",
    "  config = {\n",
    "  \"dataset\": dataset_name,\n",
    "  \"model\": \"CVAE\",\n",
    "  \"encoded_dim\": encoded_dim,\n",
    "  \"kl_coefficient\": kl_coefficient,\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epoch_count,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"patience\": patience,\n",
    "  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "history = cvae.fit([train_x,train_y_one_hot],\n",
    "                   validation_data=([val_x,val_y_one_hot],None),\n",
    "                   epochs=epoch_count,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[early_stop, WandbCallback(save_weights_only=True)  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Val_Plot(history.history['loss'][1:],\n",
    "               history.history['val_loss'][1:],\n",
    "               history.history['reconstruction_loss'][1:],\n",
    "               history.history['val_reconstruction_loss'][1:],\n",
    "               history.history['kl_loss'][1:],\n",
    "               history.history['val_kl_loss'][1:]\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward the port 6006 on server on 12006 on  my machine\n",
    "# ssh -N -L 16006:127.0.0.1:6006 nicolas.derus2@137.204.48.211\n",
    "# access with http://127.0.0.1:16006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cvae.evaluate([test_x, test_y_one_hot],None,\n",
    "            batch_size=batch_size,verbose=0)\n",
    "print('Test loss: {:.3f}'.format(test_loss[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict([train_x, train_y_one_hot])\n",
    "test_x_mean, test_log_var = cvae.encoder.predict([test_x, test_y_one_hot])\n",
    "val_x_mean, val_log_var = cvae.encoder.predict([val_x, val_y_one_hot])\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim > 2:\n",
    "    from sklearn import manifold\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    train_x_tsne = tsne.fit_transform(train_x_mean[:2000])\n",
    "    test_x_tsne = tsne.fit_transform(test_x_mean[:2000])\n",
    "    val_x_tsne = tsne.fit_transform(val_x_mean[:2000])\n",
    "    plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:2000], test_y[:2000] ,val_y[:2000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "            [train_y, test_y ,val_y],\n",
    "            ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "    plot_2d_data( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, train_y_one_hot)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, test_y_one_hot)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, val_y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 10\n",
    "\n",
    "fig, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  #axs[0, i].set_title(train_y[random_idx])\n",
    "  axs[0, i].set_title( labels[int(train_y[random_idx])]  )\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "wandb.log({\"Reconstructions\": wandb.Image(plt)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cvae.latent_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim),\n",
    "                             mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 3\n",
    "digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cond = reparametrization(z_mean=0, z_log_var=0.2, input_label = b)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[0].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 2\n",
    "_, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "    digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "    a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "    b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "    z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    digit_0 = decoded_x[0].reshape(input_shape) \n",
    "    digit_1 = decoded_x[1].reshape(input_shape) \n",
    "    axs[0, i].imshow(digit_0)\n",
    "    axs[0, i].axis('off')\n",
    "    #axs[0, i].set_title(digit_label)\n",
    "    axs[0, i].set_title( labels[digit_label]  )\n",
    "    axs[1, i].imshow(digit_1)\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title( labels[digit_label]  )\n",
    "wandb.log({\"Generations\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_interpolation(digit_label=1):\n",
    "  n = 10 # number of images per row and column\n",
    "  limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "  digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "  a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "  grid_x = np.linspace(-limit,limit, n) \n",
    "  grid_y = np.linspace(limit,-limit, n)\n",
    "\n",
    "  generated_images=[]\n",
    "  for i, yi in enumerate(grid_y):\n",
    "    single_row_generated_images=[]\n",
    "    for j, xi in enumerate(grid_x):\n",
    "      random_sample = np.array([[xi, yi]])\n",
    "      digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "      a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "      b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "      z_cond = reparametrization(z_mean=0, z_log_var=0.0, input_label = b)\n",
    "      decoded_x = cvae.decoder.predict(z_cond)\n",
    "      single_row_generated_images.append(decoded_x[0].reshape(original_image_shape))\n",
    "    generated_images.append(single_row_generated_images)      \n",
    "  plot_generated_images(generated_images,n,n,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = category_count\n",
    "if (encoded_dim == 2 & label_size <= 10):\n",
    "    for i in range(label_size):\n",
    "        latent_space_interpolation(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.save_weights('weights/cvae_toy.h5')\n",
    "cvae_encoder.save('models/cvae_encoder_toy.h5')\n",
    "cvae_decoder.save('models/cvae_decoder_toy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True) # TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "  n = 10  # number of images per row and column\n",
    "  limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "  first_dim_const= 0  # constant value of the second latent dimension\n",
    "\n",
    "  grid_y = np.linspace(-limit,limit, n) \n",
    "\n",
    "  generated_images=[]\n",
    "  for digit_label in range(category_count):\n",
    "    digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "    \n",
    "    single_row_generated_images=[]\n",
    "    for i, yi in enumerate(grid_y):\n",
    "      random_sample = np.array([[first_dim_const, yi]])\n",
    "      z_cond = sampling(z_mean=random_sample, z_log_var=0.3,\n",
    "                      input_label = digit_label_one_hot )\n",
    "      decoded_x = cvae_decoder.predict(z_cond)\n",
    "      single_row_generated_images.append(decoded_x[0].reshape(input_shape))\n",
    "    generated_images.append(single_row_generated_images)      \n",
    "\n",
    "  wandb.log({\"Interpolations\": wandb.Image(plt)})\n",
    "  plot_generated_images(generated_images,n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvae.built = True\n",
    "#cvae.load_weights('weights/vae_toy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model):\n",
    "    test = test_x[1]\n",
    "    plt.imshow(test)\n",
    "    #test = image.img_to_array(test)\n",
    "    test = np.expand_dims(test, axis=0)\n",
    "    test.shape\n",
    "    test_label = test_y_one_hot[0]\n",
    "    img_tensor = [test, test_label]\n",
    "    from keras import models\n",
    "\n",
    "    # Extracts the outputs of the top 8 layers:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    layer_outputs = []\n",
    "    layer_names = []\n",
    "    for layer in model.layers[1:]:\n",
    "        \n",
    "        try: \n",
    "            layer_outputs.append(layer.get_output_at(1))\n",
    "            layer_names.append(layer.name)\n",
    "        \n",
    "        except:\n",
    "            layer_outputs.append(layer.output)\n",
    "            layer_names.append(layer.name)\n",
    "\n",
    "    # Creates a model that will return these outputs, given the model input:\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # This will return a list of 5 Numpy arrays:\n",
    "    # one array per layer activation\n",
    "    if 'encoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        activations = activation_model.predict(conditional_input) #for encoder\n",
    "\n",
    "    if 'decoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        input_label = np.expand_dims(input_label, axis=0)\n",
    "        z_mean, z_log_var = cvae.encoder(conditional_input)\n",
    "        z_cond = cvae.sampling(z_mean, z_log_var, input_label)\n",
    "        \n",
    "        activations = activation_model.predict(z_cond) #for decoder\n",
    "    \n",
    "    for activation, name in zip(activations[0:], layer_names[0:]):\n",
    "        print(name)\n",
    "        print(activation.shape)\n",
    "    \n",
    "    for counter, (activation, name) in enumerate(zip(activations[0:], layer_names[0:])):\n",
    "        print(name)\n",
    "        plot_filters(activation, name, counter, model_name=model.name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_x[1]\n",
    "plt.imshow(test)\n",
    "#test = image.img_to_array(test)\n",
    "test = np.expand_dims(test, axis=0)\n",
    "test.shape\n",
    "test_label = test_y_one_hot[0]\n",
    "img_tensor = [test, test_label]\n",
    "input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_img.shape)\n",
    "print(input_label.shape)\n",
    "print(conditional_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [input_img, input_label]\n",
    "image_size = [28, 28, 1]\n",
    "input_img = layers.InputLayer(input_shape=image_size,\n",
    "                            dtype ='float32')(inputs[0])\n",
    "input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                dtype ='float32')(inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "print(labels)\n",
    "labels = tf.cast(labels, dtype='float32')\n",
    "print(labels)\n",
    "ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "print(ones.shape)\n",
    "labels = ones * labels \n",
    "print(labels)\n",
    "conditional_input = layers.Concatenate(axis=3)([input_img, labels]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img[:,:,:,3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_filters(activation_layer, layer_name, counter, model_name):\n",
    "    if len(activation_layer.shape) == 2: # if flat layer\n",
    "        print('flat')\n",
    "        return None\n",
    "        if activation_layer.shape[1] == 1875:\n",
    "            activation_layer = activation_layer.reshape(1, 25, 25, 3)\n",
    "        if activation_layer.shape[1] == 1024:\n",
    "           activation_layer = activation_layer.reshape(1, 16, 16, 4)\n",
    "        if activation_layer.shape[1] == 512:\n",
    "           activation_layer = activation_layer.reshape(1, 8, 8, 8)\n",
    "\n",
    "    n = math.floor(np.sqrt(activation_layer.shape[3]))\n",
    "\n",
    "    if int(n + 0.5) ** 2 == activation_layer.shape[3]:\n",
    "\n",
    "        m = n\n",
    "    else:\n",
    "        m = math.floor(activation_layer.shape[3] / n)\n",
    "\n",
    "    if activation_layer.shape[3] == 1:\n",
    "        fig, ax = plt.subplots(1, 1, sharex='col', sharey='row',\n",
    "                                figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "\n",
    "        ax.imshow(activation_layer[0,:, :, 0], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "        return None   \n",
    "\n",
    "            \n",
    "    if n == 1:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, sharex='col', sharey='row',figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(activation_layer[0,:, :, i], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "       \n",
    "        return None   \n",
    "\n",
    "    fig, ax = plt.subplots(n, m, sharex='col', sharey='row',figsize=(15, 15))\n",
    "    fig.suptitle(layer_name)\n",
    "    \n",
    " \n",
    "\n",
    "    filter_counter = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(activation_layer[0,:, :, filter_counter], cmap='viridis')\n",
    "            filter_counter += 1\n",
    "            if filter_counter == (activation_layer.shape[3] ):\n",
    "                break\n",
    "\n",
    "    wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations( cvae.encoder)\n",
    "visualize_activations(cvae.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cvae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    #img = img[ 25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conditional_input( inputs, label_size=10): \n",
    "  \n",
    "        image_size = [input_shape[0], input_shape[1], input_shape[2]]\n",
    "\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                        dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nth_filter_loss(filter_index, layer_name):\n",
    "    \"\"\"\n",
    "    We build a loss function that maximizes the activation\n",
    "    of the nth filter of the layer considered\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a connection between the input and the target layer\n",
    "    \n",
    "    submodel = tf.keras.models.Model([model.inputs[0]],\n",
    "                                     [model.get_layer(layer_name).output])\n",
    "\n",
    "# Initiate random noise\n",
    "\n",
    "    input_img_data = np.random.random((1, input_shape[0], input_shape[1],\n",
    "                                     input_shape[2]))\n",
    "\n",
    "    input_img_data =(input_img_data - 0.5) * 0.25\n",
    "    # Cast random noise from np.float64 to tf.float32 Variable\n",
    "    input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "\n",
    "    data = [input_img_data, train_y_one_hot[0]]\n",
    "    _, _, conditional_input_img = filter_conditional_input(data) \n",
    "    conditional_input_img= tf.Variable(tf.cast(conditional_input_img,\n",
    "                                         tf.float32))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = submodel(conditional_input_img)\n",
    "            loss_value = tf.reduce_mean(outputs[:, 2:-2, 2:-2, filter_index]) #removed borders in loss\n",
    "        grads = tape.gradient(loss_value, conditional_input_img)\n",
    "        normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads)))\n",
    "                                   + 1e-5)\n",
    "        #normalized_grads = tf.math.l2_normalize(grads)\n",
    "        conditional_input_img.assign_add(normalized_grads * step_size)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    #iterate = K.function([input_img], [loss_value, grads])\n",
    "\n",
    "    if loss_value > 0:\n",
    "        \n",
    "        #img = conditional_input_img.numpy().astype(np.float64)\n",
    "        #img = img.squeeze()\n",
    "        #img = deprocess_image(img) / 255.\n",
    "        img = conditional_input_img.numpy().astype(np.float64)\n",
    "        img = img.squeeze()\n",
    "        img = deprocess_image(img)\n",
    "        kept_filters.append((img, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the generated pictures for each filter.\n",
    "img_width = input_shape[0]\n",
    "img_height = input_shape[1]\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "print(input_img.shape)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "#layer_dict = dict([(layer.name, layer) for layer in model.layers[0:]])\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_filters = [layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "step_size = 10.\n",
    "kept_filters = []\n",
    "filters_dict = dict()\n",
    "for layer_name in layers_filters:\n",
    "    if 'conv' in layer_name:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        print('Processing filter for layer:', layer_name)\n",
    "        for filter_index in range(min(layer.output.shape[-1], 100)):\n",
    "            # print('Processing filter %d' % filter_index)\n",
    "            build_nth_filter_loss(filter_index, layer_name)\n",
    "        filters_dict[layer.name] = kept_filters\n",
    "        kept_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "def stich_filters(kept_filters, layer_name):\n",
    "    # By default, we will stich the best 64 (n*n) filters on a 8 x 8 grid.\n",
    "    n = int(np.sqrt(len(kept_filters)))\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top 64 filters.\n",
    "    kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    kept_filters = kept_filters[:n * n]\n",
    "\n",
    "    # build a black picture with enough space for\n",
    "    # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "    margin = 5\n",
    "    \n",
    "    width = n * img_width + (n - 1) * margin\n",
    "    height = n * img_height + (n - 1) * margin\n",
    "\n",
    "    stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "    # fill the picture with our saved filters\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, _ = kept_filters[i * n + j]\n",
    "            width_margin = (img_width + margin) * i\n",
    "            height_margin = (img_height + margin) * j\n",
    "            stitched_filters[\n",
    "                width_margin: width_margin + img_width,\n",
    "                height_margin: height_margin + img_height, :] = img[:, :, :3] \n",
    "\n",
    "    wandb.log({\"Filters\": wandb.Image(stitched_filters, caption=\"{}_{}\".format(model.name, layer_name)) })\n",
    "    # save the result to disk\n",
    "    save_img(\"reports/\" +directory + '/filters/{}_stitched_filters_{}.png'.format(model.name,\n",
    "             layer_name), stitched_filters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, kept_filters in filters_dict.items():\n",
    "    print('Stiching filters for {}'.format(layer_name))\n",
    "    stich_filters(kept_filters, layer_name)\n",
    "    print('number of filters kept:', len(kept_filters))\n",
    "    print('Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.cvae import CVAE\n",
    "#CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report activations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
