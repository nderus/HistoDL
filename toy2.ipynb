{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10):\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "    labels = ones * labels\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count=10 \n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size=10000\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y,\n",
    "                            test_size = val_size,random_state = 1,shuffle=True)\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Validation data flatten shape: ',val_x.shape)\n",
    "print('Validation label shape: ',val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_x.shape) == 3:\n",
    "    train_x = np.expand_dims(train_x, axis=3)\n",
    "    test_x = np.expand_dims(test_x, axis=3)\n",
    "    val_x = np.expand_dims(val_x, axis=3)\n",
    "\n",
    "print('Train shape: ',train_x.shape)\n",
    "print('Test shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.0\n",
    "val_x = val_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_x.shape[2] == 1:\n",
    "    train_x = (train_x * 2) - 1\n",
    "    val_x = (val_x * 2) - 1\n",
    "    test_x = (test_x * 2) -1\n",
    "    print('Min value: ',train_x.min())\n",
    "    print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_shape=(train_x.shape[1], train_x.shape[2])\n",
    "\n",
    "train_x_flatten=np.reshape(train_x,(train_x.shape[0],-1))\n",
    "val_x_flatten=np.reshape(val_x,(val_x.shape[0],-1))\n",
    "test_x_flatten=np.reshape(test_x,(test_x.shape[0],-1))\n",
    "\n",
    "print('Train data flatten shape: ',train_x_flatten.shape)\n",
    "print('Validation data flatten shape: ',val_x_flatten.shape)\n",
    "print('Test data flatten shape: ',test_x_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(vae_input, vae_ouput,mu, log_var ,kl_coefficient):\n",
    "  #Reconstruction loss\n",
    "  reconstruction_loss = keras.losses.mean_squared_error(vae_input,vae_ouput)* train_x_flatten.shape[1]\n",
    "\n",
    "  #Regularization loss\n",
    "  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n",
    "\n",
    "  #Combined loss\n",
    "  return reconstruction_loss + kl_coefficient*kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = to_categorical(train_y,category_count)\n",
    "val_y_one_hot=to_categorical(val_y,category_count)\n",
    "test_y_one_hot=to_categorical(test_y,category_count)\n",
    "\n",
    "print('Train label one hot encoding shape: ',train_y_one_hot.shape)\n",
    "print('Validation label one hot encoding shape: ',val_y_one_hot.shape)\n",
    "print('Test label one hot encoding shape: ',test_y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder( input_shape = (28, 28, 1),  label_size=10, encoded_dim = 2): \n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape[0],\n",
    "            input_shape[1], input_shape[2] + label_size), dtype='float32',name='Input')\n",
    "    #inputs = layers.Input(shape = input_shape)\n",
    "    #labels_inputs = layers.Input(shape = (50, 50, 2))\n",
    "    #encoder_inputs = layers.Concatenate()([inputs, labels_inputs])\n",
    "\n",
    "\n",
    "    #block 1\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(inputs)\n",
    "\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # block 2\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2,name='S4')(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name='block3_conv2')(x)    \n",
    "                    \n",
    "    x = layers.Conv2D(filters=5, kernel_size=5,strides=1,padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(encoded_dim * 2)(x)\n",
    "    mu = layers.Dense(encoded_dim, name='mu')(y)\n",
    "    log_var = layers.Dense(encoded_dim, name='log_var')(y)\n",
    "    \n",
    "\n",
    "    model = keras.Model(inputs, [mu, log_var], name='encoder')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_shape, encoded_dim = 2,label_size=10): \n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(encoded_dim + label_size,) , name='decoder_input')\n",
    "    x = layers.Dense(encoded_dim)\n",
    "    x = layers.Dense(encoded_dim * 2)\n",
    "    #x = layers.Dense(14*14*64)(decoder_inputs)\n",
    "    x = layers.Dense(input_shape[0]/2 * input_shape[1]/2 *64)(decoder_inputs)\n",
    "    x = layers.Reshape(target_shape=(int(input_shape[0]/2),\n",
    "                     int(input_shape[1]/2), 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block4_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block4_conv2')(x)  \n",
    "    \n",
    "    # block 2\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv2')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block6_conv1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block6_conv2')(x)\n",
    "                                   \n",
    "    outputs = layers.Conv2DTranspose(filters=input_shape[-1], kernel_size=2, strides=1, activation='sigmoid',padding='same')(x)\n",
    "\n",
    "    model = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder = encoder(encoded_dim = encoded_dim, input_shape = input_shape)\n",
    "cvae_decoder = decoder(encoded_dim = encoded_dim, input_shape = input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, shape, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.shape = shape\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        #\n",
    "        self.v_total_loss_tracker = keras.metrics.Mean(name=\"v_total_loss\")\n",
    "        self.v_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"v_reconstruction_loss\")\n",
    "        self.v_kl_loss_tracker = keras.metrics.Mean(name=\"v_kl_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "    \n",
    "    def conditional_input(self, inputs, label_size=10): \n",
    "        image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                      dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "    def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "        eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                               mean=0., stddev=1.0, name='epsilon')\n",
    "        z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1)\n",
    "        return z_cond\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "            z_mean, z_log_var = self.encoder(conditional_input)\n",
    "            z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "            reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "                                    tf.keras.backend.flatten(reconstruction))     \n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                      - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "            total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "            total_loss = tf.reduce_mean(total_loss) \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(reconstruction)) # over weighted MSE    \n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                  - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return{\n",
    "            'loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_coefficient=1\n",
    "cvae = CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)\n",
    "cvae.built = True\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_input = cvae.encoder.input[0]\n",
    "cvae_output = cvae.decoder.output\n",
    "mu = cvae.encoder.get_layer('mu').output\n",
    "log_var = cvae.encoder.get_layer('log_var').output\n",
    "\n",
    "cvae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 100\n",
    "patience = 5\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "history = cvae.fit([train_x,train_y_one_hot],\n",
    "                   validation_data=([val_x,val_y_one_hot],None),\n",
    "                   epochs=epoch_count,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cvae.evaluate([test_x, test_y_one_hot],None,\n",
    "            batch_size=batch_size,verbose=0)\n",
    "print('Test loss: {:.3f}'.format(test_loss[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10)\n",
      "(40000, 32, 32, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_size = 10\n",
    "_, input_label_train, train_input = cvae.conditional_input([train_x, train_y_one_hot])\n",
    "_, input_label_test, test_input = cvae.conditional_input([test_x, test_y_one_hot])\n",
    "_, input_label_val, val_input = cvae.conditional_input([val_x, val_y_one_hot])\n",
    "\n",
    "\n",
    "print(input_label_train.shape)\n",
    "print(train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100)\n",
      "(40000, 100)\n"
     ]
    }
   ],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict(train_input)\n",
    "test_x_mean, test_log_var = cvae.encoder.predict(test_input)\n",
    "val_x_mean, val_log_var = cvae.encoder.predict(val_input)\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "train_x_tsne = tsne.fit_transform(train_x_mean[:1000])\n",
    "test_x_tsne = tsne.fit_transform(test_x_mean[:1000])\n",
    "val_x_tsne = tsne.fit_transform(val_x_mean[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:1000], test_y[:1000] ,val_y[:1000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, input_label_train)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, input_label_test)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, input_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 5\n",
    "\n",
    "_, axs = plt.subplots(2, image_count, figsize=(15, 5))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, train_x.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  axs[0, i].set_title(train_y[random_idx])\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim), mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_label = 5\n",
    "digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_label_one_hot = train_y_one_hot[:batch_size]\n",
    "inputs = [train_x[:2], digit_label_one_hot] #train_x a placeholder for dims\n",
    "_, labels,_ = cvae.conditional_input(inputs)\n",
    "z_cond = reparametrization(z_mean=0., z_log_var=.3, input_label = b)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[1].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit lable one hot encoding:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 1 [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000029vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m#digit_label_one_hot = train_y_one_hot[batch_size]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000029vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDigit lable one hot encoding: \u001b[39m\u001b[39m'\u001b[39m, digit_label_one_hot)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000029vscode-remote?line=8'>9</a>\u001b[0m z_cond \u001b[39m=\u001b[39m sampling(z_mean\u001b[39m=\u001b[39;49m\u001b[39m1.\u001b[39;49m, z_log_var\u001b[39m=\u001b[39;49m\u001b[39m.3\u001b[39;49m, input_label \u001b[39m=\u001b[39;49m labels )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000029vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(z_cond)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000029vscode-remote?line=10'>11</a>\u001b[0m decoded_x \u001b[39m=\u001b[39m cvae_decoder\u001b[39m.\u001b[39mpredict(z_cond)\n",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb Cell 6'\u001b[0m in \u001b[0;36msampling\u001b[0;34m(z_mean, z_log_var, input_label)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m eps \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(tf\u001b[39m.\u001b[39mshape(z_log_var), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000009vscode-remote?line=3'>4</a>\u001b[0m                         mean\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, stddev\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000009vscode-remote?line=4'>5</a>\u001b[0m z \u001b[39m=\u001b[39m z_mean \u001b[39m+\u001b[39m tf\u001b[39m.\u001b[39mexp(z_log_var \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m eps\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000009vscode-remote?line=5'>6</a>\u001b[0m z_cond \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat([z, input_label], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2.ipynb#ch0000009vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m z_cond\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=7104'>7105</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=7105'>7106</a>\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=7106'>7107</a>\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 1 [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "digit_label = 1\n",
    "batch_size = 2\n",
    "random_sample = np.random.normal(loc = 0, scale = .3, size= encoded_dim)\n",
    "random_sample = random_sample.astype('double')\n",
    "#digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "digit_label_one_hot = train_y_one_hot[:batch_size]\n",
    "print('Digit lable one hot encoding: ', digit_label_one_hot)\n",
    "\n",
    "z_cond = reparametrization(z_mean=1., z_log_var=.3, input_label = labels )\n",
    "print(z_cond)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[0].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # number of images per row and column\n",
    "limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "first_dim_const= 0  # constant value of the second latent dimension\n",
    "\n",
    "grid_y = np.linspace(-limit,limit, n) \n",
    "\n",
    "generated_images=[]\n",
    "for digit_label in range(category_count):\n",
    "  digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "  \n",
    "  single_row_generated_images=[]\n",
    "  for i, yi in enumerate(grid_y):\n",
    "    random_sample = np.array([[first_dim_const, yi]])\n",
    "    z_cond = sampling(z_mean=random_sample, z_log_var=0.3, input_label = digit_label_one_hot )\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    single_row_generated_images.append(decoded_x[0].reshape(input_shape))\n",
    "  generated_images.append(single_row_generated_images)      \n",
    "\n",
    "plot_generated_images(generated_images,n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
