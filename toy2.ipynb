{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10):\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "    labels = ones * labels\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count=10 \n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size=10000\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y,\n",
    "                            test_size = val_size,random_state = 1,shuffle=True)\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Validation data flatten shape: ',val_x.shape)\n",
    "print('Validation label shape: ',val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_x.shape) == 3:\n",
    "    train_x = np.expand_dims(train_x, axis=3)\n",
    "    test_x = np.expand_dims(test_x, axis=3)\n",
    "    val_x = np.expand_dims(val_x, axis=3)\n",
    "\n",
    "print('Train shape: ',train_x.shape)\n",
    "print('Test shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.0\n",
    "val_x = val_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_x.shape[2] == 1:\n",
    "    train_x = (train_x * 2) - 1\n",
    "    val_x = (val_x * 2) - 1\n",
    "    test_x = (test_x * 2) -1\n",
    "    print('Min value: ',train_x.min())\n",
    "    print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_shape=(train_x.shape[1], train_x.shape[2])\n",
    "\n",
    "train_x_flatten=np.reshape(train_x,(train_x.shape[0],-1))\n",
    "val_x_flatten=np.reshape(val_x,(val_x.shape[0],-1))\n",
    "test_x_flatten=np.reshape(test_x,(test_x.shape[0],-1))\n",
    "\n",
    "print('Train data flatten shape: ',train_x_flatten.shape)\n",
    "print('Validation data flatten shape: ',val_x_flatten.shape)\n",
    "print('Test data flatten shape: ',test_x_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(vae_input, vae_ouput,mu, log_var ,kl_coefficient):\n",
    "  #Reconstruction loss\n",
    "  reconstruction_loss = keras.losses.mean_squared_error(vae_input,vae_ouput)* train_x_flatten.shape[1]\n",
    "\n",
    "  #Regularization loss\n",
    "  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n",
    "\n",
    "  #Combined loss\n",
    "  return reconstruction_loss + kl_coefficient*kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = to_categorical(train_y,category_count)\n",
    "val_y_one_hot=to_categorical(val_y,category_count)\n",
    "test_y_one_hot=to_categorical(test_y,category_count)\n",
    "\n",
    "print('Train label one hot encoding shape: ',train_y_one_hot.shape)\n",
    "print('Validation label one hot encoding shape: ',val_y_one_hot.shape)\n",
    "print('Test label one hot encoding shape: ',test_y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder( input_shape = (28, 28, 1),  label_size=10, encoded_dim = 2): \n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape[0],\n",
    "            input_shape[1], input_shape[2] + label_size), dtype='float32',name='Input')\n",
    "    #inputs = layers.Input(shape = input_shape)\n",
    "    #labels_inputs = layers.Input(shape = (50, 50, 2))\n",
    "    #encoder_inputs = layers.Concatenate()([inputs, labels_inputs])\n",
    "\n",
    "\n",
    "    #block 1\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(inputs)\n",
    "\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # block 2\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2,name='S4')(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name='block3_conv2')(x)    \n",
    "                    \n",
    "    x = layers.Conv2D(filters=5, kernel_size=5,strides=1,padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(encoded_dim * 2)(x)\n",
    "    mu = layers.Dense(encoded_dim, name='mu')(y)\n",
    "    log_var = layers.Dense(encoded_dim, name='log_var')(y)\n",
    "    \n",
    "\n",
    "    model = keras.Model(inputs, [mu, log_var], name='encoder')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_shape, encoded_dim = 2,label_size=10): \n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(encoded_dim + label_size,) , name='decoder_input')\n",
    "    x = layers.Dense(encoded_dim)\n",
    "    x = layers.Dense(encoded_dim * 2)\n",
    "    #x = layers.Dense(14*14*64)(decoder_inputs)\n",
    "    x = layers.Dense(input_shape[0]/2 * input_shape[1]/2 *64)(decoder_inputs)\n",
    "    x = layers.Reshape(target_shape=(int(input_shape[0]/2),\n",
    "                     int(input_shape[1]/2), 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block4_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block4_conv2')(x)  \n",
    "    \n",
    "    # block 2\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv2')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block6_conv1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block6_conv2')(x)\n",
    "                                   \n",
    "    outputs = layers.Conv2DTranspose(filters=input_shape[-1], kernel_size=2, strides=1, activation='sigmoid',padding='same')(x)\n",
    "\n",
    "    model = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder = encoder(encoded_dim = encoded_dim, input_shape = input_shape)\n",
    "cvae_decoder = decoder(encoded_dim = encoded_dim, input_shape = input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, shape, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.shape = shape\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        #\n",
    "        self.v_total_loss_tracker = keras.metrics.Mean(name=\"v_total_loss\")\n",
    "        self.v_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"v_reconstruction_loss\")\n",
    "        self.v_kl_loss_tracker = keras.metrics.Mean(name=\"v_kl_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "    \n",
    "    def conditional_input(self, inputs, label_size=10): \n",
    "        image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                      dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "    def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "        eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                               mean=0., stddev=1.0, name='epsilon')\n",
    "        z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1)\n",
    "        return z_cond\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "            z_mean, z_log_var = self.encoder(conditional_input)\n",
    "            z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "            reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "                                    tf.keras.backend.flatten(reconstruction))     \n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                      - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "            total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "            total_loss = tf.reduce_mean(total_loss) \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(reconstruction)) # over weighted MSE    \n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                  - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return{\n",
    "            'loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_coefficient=1\n",
    "cvae = CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)\n",
    "cvae.built = True\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_input = cvae.encoder.input[0]\n",
    "cvae_output = cvae.decoder.output\n",
    "mu = cvae.encoder.get_layer('mu').output\n",
    "log_var = cvae.encoder.get_layer('log_var').output\n",
    "\n",
    "cvae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 100\n",
    "patience = 5\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "history = cvae.fit([train_x,train_y_one_hot],\n",
    "                   validation_data=([val_x,val_y_one_hot],None),\n",
    "                   epochs=epoch_count,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cvae.evaluate([test_x, test_y_one_hot],None,\n",
    "            batch_size=batch_size,verbose=0)\n",
    "print('Test loss: {:.3f}'.format(test_loss[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_size = 10\n",
    "_, input_label_train, train_input = cvae.conditional_input([train_x, train_y_one_hot])\n",
    "_, input_label_test, test_input = cvae.conditional_input([test_x, test_y_one_hot])\n",
    "_, input_label_val, val_input = cvae.conditional_input([val_x, val_y_one_hot])\n",
    "\n",
    "\n",
    "print(input_label_train.shape)\n",
    "print(train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict(train_input)\n",
    "test_x_mean, test_log_var = cvae.encoder.predict(test_input)\n",
    "val_x_mean, val_log_var = cvae.encoder.predict(val_input)\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "train_x_tsne = tsne.fit_transform(train_x_mean[:1000])\n",
    "test_x_tsne = tsne.fit_transform(test_x_mean[:1000])\n",
    "val_x_tsne = tsne.fit_transform(val_x_mean[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:1000], test_y[:1000] ,val_y[:1000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, input_label_train)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, input_label_test)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, input_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 5\n",
    "\n",
    "_, axs = plt.subplots(2, image_count, figsize=(15, 5))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, train_x.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  axs[0, i].set_title(train_y[random_idx])\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim), mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 5\n",
    "digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label_one_hot = train_y_one_hot[:batch_size]\n",
    "inputs = [train_x[:2], digit_label_one_hot] #train_x a placeholder for dims\n",
    "_, labels,_ = cvae.conditional_input(inputs)\n",
    "z_cond = reparametrization(z_mean=0., z_log_var=.3, input_label = b)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[1].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # number of images per row and column\n",
    "limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "first_dim_const= 0  # constant value of the second latent dimension\n",
    "\n",
    "grid_y = np.linspace(-limit,limit, n) \n",
    "\n",
    "generated_images=[]\n",
    "for digit_label in range(category_count):\n",
    "  digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "  \n",
    "  single_row_generated_images=[]\n",
    "  for i, yi in enumerate(grid_y):\n",
    "    random_sample = np.array([[first_dim_const, yi]])\n",
    "    z_cond = sampling(z_mean=random_sample, z_log_var=0.3, input_label = digit_label_one_hot )\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    single_row_generated_images.append(decoded_x[0].reshape(input_shape))\n",
    "  generated_images.append(single_row_generated_images)      \n",
    "\n",
    "plot_generated_images(generated_images,n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
