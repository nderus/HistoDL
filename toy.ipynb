{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data flatten shape:  (60000, 28, 28)\n",
      "Train label shape:  (60000,)\n",
      "Test data flatten shape:  (10000, 28, 28)\n",
      "Test label shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "category_count=10 #Number of digit categories\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_count=10\n",
    "\n",
    "_, axs = plt.subplots(1, image_count,figsize=(15, 10))\n",
    "for i in range(image_count):\n",
    "  random_idx=random.randint(0,train_x.shape[0])\n",
    "  axs[i].imshow(train_x[random_idx],cmap='gray')\n",
    "  axs[i].axis('off')\n",
    "  axs[i].set_title(train_y[random_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data flatten shape:  (50000, 28, 28)\n",
      "Train label shape:  (50000,)\n",
      "Validation data flatten shape:  (10000, 28, 28)\n",
      "Validation label shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "val_size=10000\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = val_size,random_state = 1,shuffle=True)\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Validation data flatten shape: ',val_x.shape)\n",
    "print('Validation label shape: ',val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (50000, 28, 28, 1)\n",
      "Test shape:  (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "if len(train_x.shape) == 3:\n",
    "    train_x = np.expand_dims(train_x, axis=3)\n",
    "    test_x = np.expand_dims(test_x, axis=3)\n",
    "    val_x = np.expand_dims(val_x, axis=3)\n",
    "\n",
    "print('Train shape: ',train_x.shape)\n",
    "print('Test shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value:  0.0\n",
      "Max value:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x = train_x/255.0\n",
    "val_x = val_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_x.shape[2] == 1:\n",
    "    train_x = (train_x * 2) - 1\n",
    "    val_x = (val_x * 2) - 1\n",
    "    test_x = (test_x * 2) -1\n",
    "    print('Min value: ',train_x.min())\n",
    "    print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "    \"\"\"Reparameterization trick. Instead of sampling from Q(z|X), \n",
    "    sample eps = N(0,I) z = z_mean + sqrt(var)*eps.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    args: list of Tensors\n",
    "        Mean and log of variance of Q(z|X)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    z: Tensor\n",
    "        Sampled latent vector\n",
    "    \"\"\"\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32, mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_CVAE( input_shape = (32, 32, 3),  label_size=2, latent_dim = 2): \n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape[0],\n",
    "            input_shape[1], input_shape[2] + label_size), dtype='float32',name='Input')\n",
    "    #inputs = layers.Input(shape = input_shape)\n",
    "    #labels_inputs = layers.Input(shape = (50, 50, 2))\n",
    "    #encoder_inputs = layers.Concatenate()([inputs, labels_inputs])\n",
    "\n",
    "\n",
    "    #block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(inputs)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    \n",
    "    # block 2\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2,name='S4')(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name='block3_conv2')(x)    \n",
    "                    \n",
    "    x = layers.Conv2D(filters=5, kernel_size=5,strides=1,padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(latent_dim * 2)(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(y)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(y)\n",
    "    #z_cond = layers.Lambda(sampling, name='z')([z_mean, z_log_var, input_label]) #reparametrization trick\n",
    "    model = keras.Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_CVAE(latent_dim = 2,label_size=2): \n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim + label_size,) , name='decoder_input')\n",
    "    x = layers.Dense(14*14*1)(decoder_inputs) # if latent_dim < 25*25*3\n",
    "    x = layers.Reshape(target_shape=(14, 14, 1))(x)\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block4_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block4_conv2')(x)  \n",
    "    \n",
    "    # block 2\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv2')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='up_block6_conv1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    name='up_block6_conv2')(x)\n",
    "                      \n",
    "                      \n",
    "    outputs = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, activation='sigmoid',padding='same')(x)\n",
    "   # outputs = layers.Reshape(target_shape=(50, 50, 3), name='output')(x)\n",
    "    model = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 28, 28, 11)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 28, 28, 64)   6400        ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 28, 28, 64)   36928       ['block1_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 28, 28, 32)   18464       ['block1_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 28, 28, 32)   9248        ['block2_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " S4 (MaxPooling2D)              (None, 14, 14, 32)   0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 14, 14, 32)  128         ['S4[0][0]']                     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 14, 14, 16)   4624        ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 14, 14, 16)   2320        ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 14, 14, 5)    2005        ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 980)          0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4)            3924        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            10          ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            10          ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84,061\n",
      "Trainable params: 83,997\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_encoder = encoder_CVAE(input_shape = (28, 28, 1), latent_dim = 2, label_size=category_count)\n",
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 12)]              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 196)               2548      \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 14, 14, 1)         0         \n",
      "                                                                 \n",
      " up_block4_conv1 (Conv2DTran  (None, 14, 14, 16)       160       \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block4_conv2 (Conv2DTran  (None, 14, 14, 16)       2320      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block5_conv1 (Conv2DTran  (None, 14, 14, 32)       4640      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block5_conv2 (Conv2DTran  (None, 14, 14, 32)       9248      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 28, 28, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " up_block6_conv1 (Conv2DTran  (None, 28, 28, 64)       18496     \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block6_conv2 (Conv2DTran  (None, 28, 28, 64)       36928     \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        577       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,917\n",
      "Trainable params: 74,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_decoder = decoder_CVAE(latent_dim = 2, label_size = category_count)\n",
    "cvae_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condtional vae\n",
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        #\n",
    "        self.v_total_loss_tracker = keras.metrics.Mean(name=\"v_total_loss\")\n",
    "        self.v_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"v_reconstruction_loss\")\n",
    "        self.v_kl_loss_tracker = keras.metrics.Mean(name=\"v_kl_loss\")\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def conditional_input(self, inputs, image_size=[28, 28, 1], label_size=10): #inputs should be a 2 dim array\n",
    "        input_img = layers.InputLayer(input_shape=image_size, dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ), dtype ='float32')(inputs[1])\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size]) #batch_size, 1, 1, label_size\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) #batch_size, 50, 50, label_size\n",
    "        labels = ones * labels #batch_size, 50, 50, label_size\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "            z_mean, z_log_var = self.encoder(conditional_input)\n",
    "            z_cond = sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "            reconstruction_loss = np.prod((28, 28, 1)) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(reconstruction)) # over weighted MSE    \n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "            total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "            total_loss = tf.reduce_mean(total_loss) \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "            print(data[0].shape)\n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var= self.encoder(conditional_input)\n",
    "        z_cond = sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = np.prod((32, 32, 3)) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(reconstruction)) # over weighted MSE    \n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        total_loss = reconstruction_loss + (self.beta * kl_loss)\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return{\n",
    "            'loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = to_categorical(train_y,category_count)\n",
    "val_y_one_hot=to_categorical(val_y,category_count)\n",
    "test_y_one_hot=to_categorical(test_y,category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [train_x, train_y_one_hot]\n",
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(inputs, image_size=[28,28,1], label_size=10): #inputs should be a 2 dim array\n",
    "    input_img = layers.InputLayer(input_shape=image_size, dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ), dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size]) #batch_size, 1, 1, label_size\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) #batch_size, 50, 50, label_size\n",
    "    labels = ones * labels #batch_size, 50, 50, label_size\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input\n",
    "\n",
    "\n",
    "input_img, input_label, conditional_input = conditional_input(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50000, 12])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean, z_log_var = cvae_encoder(conditional_input)\n",
    "z_cond = sampling(z_mean, z_log_var, input_label)\n",
    "z_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = cvae_decoder(z_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(50000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(reconstruction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstruction_loss = np.prod((28, 28, 1)) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(reconstruction)) # over weighted MSE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_coeff = 1\n",
    "cvae = CVAE(encoder=cvae_encoder, decoder=cvae_decoder, beta = beta_coeff)\n",
    "#vae.compile(optimizer='Adam')\n",
    "cvae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1))\n",
    "#lot_model(vae, show_shapes=True, show_layer_names=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 12)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.decoder.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 103s 203ms/step - loss: 93.7208 - reconstruction_loss: 88.9397 - kl_loss: 0.0157\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 101s 202ms/step - loss: 87.6724 - reconstruction_loss: 87.7954 - kl_loss: 4.9353e-09\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 101s 201ms/step - loss: 87.6978 - reconstruction_loss: 87.7955 - kl_loss: 2.1338e-10\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 87.9402 - reconstruction_loss: 87.7955 - kl_loss: 3.5763e-12\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 87.5605 - reconstruction_loss: 87.7954 - kl_loss: 1.1921e-12\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 87.4471 - reconstruction_loss: 87.7955 - kl_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 87.8051 - reconstruction_loss: 87.7955 - kl_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 87.7775 - reconstruction_loss: 87.7955 - kl_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 101s 201ms/step - loss: 87.9053 - reconstruction_loss: 87.7955 - kl_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 101s 201ms/step - loss: 87.7459 - reconstruction_loss: 87.7955 - kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "#model.compile( optimizer='adam')\n",
    "tf.config.run_functions_eagerly(False)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "history = cvae.fit([train_x, train_y_one_hot], train_x,\n",
    "                    epochs=epochs, batch_size=batch_size, callbacks=early_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"cvae_3\" (type CVAE).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_29174/1132900734.py\", line 20, in call  *\n            _, input_label, conditional_input = self.conditional_input(inputs)\n        File \"/tmp/ipykernel_29174/1132900734.py\", line 38, in conditional_input  *\n            ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) #batch_size, 50, 50, label_size\n    \n        TypeError: Expected int32, but got None of type 'NoneType'.\n    \n    \n    Call arguments received:\n      • inputs=('tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 10), dtype=float32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/toy.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m [train_x, train_y_one_hot]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy.ipynb#ch0000022vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m#tf.concat([z, input_label], axis=1)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy.ipynb#ch0000022vscode-remote?line=2'>3</a>\u001b[0m p \u001b[39m=\u001b[39m cvae\u001b[39m.\u001b[39;49mpredict(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"cvae_3\" (type CVAE).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_29174/1132900734.py\", line 20, in call  *\n            _, input_label, conditional_input = self.conditional_input(inputs)\n        File \"/tmp/ipykernel_29174/1132900734.py\", line 38, in conditional_input  *\n            ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) #batch_size, 50, 50, label_size\n    \n        TypeError: Expected int32, but got None of type 'NoneType'.\n    \n    \n    Call arguments received:\n      • inputs=('tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 10), dtype=float32)')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = [train_x, train_y_one_hot]\n",
    "#tf.concat([z, input_label], axis=1)\n",
    "p = cvae.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Val_Plot(loss, val_loss, reconstruction_loss, val_reconstruction_loss, kl_loss, val_kl_loss):\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize= (16,4))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(loss) + 1), loss)\n",
    "    ax1.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax1.set_title('History of Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "    ax2.plot(range(1, len(reconstruction_loss) + 1), reconstruction_loss)\n",
    "    ax2.plot(range(1, len(val_reconstruction_loss) + 1), val_reconstruction_loss)\n",
    "    ax2.set_title('History of reconstruction_loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('reconstruction_loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "    \n",
    "    ax3.plot(range(1, len(kl_loss) + 1), kl_loss)\n",
    "    ax3.plot(range(1, len(val_kl_loss) + 1), val_kl_loss)\n",
    "    ax3.set_title(' History of kl_loss')\n",
    "    ax3.set_xlabel(' Epochs ')\n",
    "    ax3.set_ylabel('kl_loss')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "     \n",
    "    \n",
    "    plt.show()\n",
    "    #fig.savefig('img/vae_loss_latent:{}_epochs:{}_beta:{}.png'.format(latent_dim, epochs, beta_coeff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Val_Plot(history.history['loss'][1:],\n",
    "               history.history['val_loss'][1:],\n",
    "               history.history['reconstruction_loss'][1:],\n",
    "               history.history['val_reconstruction_loss'][1:],\n",
    "               history.history['kl_loss'][1:],\n",
    "               history.history['val_kl_loss'][1:]\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvae.save_weights('weights/vae_toy.h5')\n",
    "cvae.built = True\n",
    "cvae.load_weights('weights/vae_toy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred):    \n",
    "    f, ax = plt.subplots(2, 10, figsize=(15, 4))\n",
    "    for i in range(10):\n",
    "        ax[0][i].imshow(np.reshape(y_true[i], (32, 32, 3)), aspect='auto')\n",
    "        ax[1][i].imshow(np.reshape(y_pred[i], (32, 32, 3)), aspect='auto')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_x[:100], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter with images instead of points\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "img_size = 32\n",
    "def imscatter(x, y, ax, imageData, zoom):\n",
    "    images = []\n",
    "    for i in range(len(x)):\n",
    "        x0, y0 = x[i], y[i]\n",
    "        # Convert to image\n",
    "        img = imageData[i]*255.\n",
    "        img = img.astype(np.uint8).reshape([img_size,img_size,3])\n",
    "        #img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "        # Note: OpenCV uses BGR and plt uses RGB\n",
    "        image = OffsetImage(img, zoom=zoom)\n",
    "        ab = AnnotationBbox(image, (x0, y0), xycoords='data', frameon=False)\n",
    "        images.append(ax.add_artist(ab))\n",
    "    \n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/despoisj/LatentSpaceVisualization/blob/master/visuals.py\n",
    "from sklearn import manifold\n",
    "\n",
    "def computeTSNEProjectionOfLatentSpace(X, X_encoded, display=True, save=True):\n",
    "    # Compute latent space representation\n",
    "    print(\"Computing latent space projection...\")\n",
    "    #X_encoded = encoder.predict(X)\n",
    "\n",
    "    # Compute t-SNE embedding of latent space\n",
    "    print(\"Computing t-SNE embedding...\")\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    X_tsne = tsne.fit_transform(X_encoded)\n",
    "\n",
    "    # Plot images according to t-sne embedding\n",
    "    if display:\n",
    "        print(\"Plotting t-SNE visualization...\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 15))\n",
    "        ax = fig.add_subplot(111, facecolor='black')\n",
    "        imscatter(X_tsne[:, 0], X_tsne[:, 1], imageData=X, ax=ax, zoom=0.5)\n",
    "        if save:\n",
    "            fig.savefig('img/t-SNE-embedding_vae_epochs:{}_beta:{}.png'.format(epochs, beta_coeff))\n",
    "        plt.show()\n",
    "    else:\n",
    "        return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = cvae.encoder.predict(inputs)\n",
    "X_encoded.shape\n",
    "#need to reshape for TSNE\n",
    "#X_encoded_flatten = X_encoded.reshape(-1,25*25*3)\n",
    "#X_encoded_flatten.shape\n",
    "X_encoded_flatten = X_encoded\n",
    "X_encoded_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeTSNEProjectionOfLatentSpace(train_x[:1000,], X_encoded_flatten, display=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReconstructedImages(X, encoder, decoder):\n",
    "    img_size = 32\n",
    "    nbSamples = X.shape[0]\n",
    "    nbSquares = int(math.sqrt(nbSamples))\n",
    "    nbSquaresHeight = 2*nbSquares\n",
    "    nbSquaresWidth = nbSquaresHeight\n",
    "    resultImage = np.zeros((nbSquaresHeight*img_size,int(nbSquaresWidth*img_size/2),X.shape[-1]))\n",
    "\n",
    "    reconstructedX = decoder.predict(encoder.predict(X)[2])\n",
    "\n",
    "    for i in range(nbSamples) :     # \n",
    "        original = X[i]\n",
    "        reconstruction = reconstructedX[i]\n",
    "        rowIndex = i%nbSquaresWidth\n",
    "        columnIndex = int((i-rowIndex)/nbSquaresHeight)\n",
    "        resultImage[rowIndex*img_size:(rowIndex+1)*img_size,columnIndex*2*img_size:(columnIndex+1)*2*img_size,:] = np.hstack([original,reconstruction])\n",
    "\n",
    "    return resultImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions for samples in dataset\n",
    "def visualizeReconstructedImages(X_train, X_test, encoder, decoder, save=False):\n",
    "    trainReconstruction = getReconstructedImages(X_train, encoder, decoder)\n",
    "    testReconstruction = getReconstructedImages(X_test, encoder, decoder)\n",
    "\n",
    "    if not save:\n",
    "        print(\"Generating 10 image reconstructions...\")\n",
    "\n",
    "    result = np.hstack([trainReconstruction,\n",
    "            np.zeros([trainReconstruction.shape[0],5,\n",
    "            trainReconstruction.shape[-1]]),\n",
    "            testReconstruction])\n",
    "    result = (result*255.).astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        fig, _ = plt.subplots(figsize=(15, 15))\n",
    "        plt.imshow(result)\n",
    "        fig.savefig('img/vae_reconstructions_epochs:{}_beta:{}.png'.format(epochs, beta_coeff))\n",
    "    else:\n",
    "        fig, _ = plt.subplots(figsize=(15, 15))\n",
    "        plt.imshow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeReconstructedImages(train_x[:100], test_x[:100], cvae_encoder, cvae_decoder, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = cvae.encoder.output_shape[0][1]\n",
    "latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.decoder.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images(decoder):    \n",
    "    fig, ax = plt.subplots(2, 10, figsize=(15, 4))\n",
    "    label = train_y_one_hot[0]\n",
    "    print(label)\n",
    "    for i in range(2):\n",
    "        for j in range(10):\n",
    "            noise = np.random.normal(loc=0, scale = 1, size=(20,latent_dim))\n",
    "                \n",
    "            noise = np.array(noise)\n",
    "            print(noise.shape)\n",
    "            print(label.shape)\n",
    "            #noise = noise.reshape(1, latent_dim)\n",
    "            input = tf.concat([noise, label], axis=1)\n",
    "            decoded = cvae_decoder.predict(input).squeeze()\n",
    "            ax[i][j].imshow( (decoded*255.).astype(np.uint8) )\n",
    "    fig.savefig('img/vae_generations_latent:{}_epochs:{}_beta:{}.png'.format(latent_dim, epochs, beta_coeff))   \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(cvae_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeneratedImages(X, y, encoder, decoder):\n",
    "    img_size = 32\n",
    "    nbSamples = X.shape[0]\n",
    "    nbSquares = int(math.sqrt(nbSamples))\n",
    "    nbSquaresHeight = 2*nbSquares\n",
    "    nbSquaresWidth = nbSquaresHeight\n",
    "    resultImage = np.zeros((nbSquaresHeight*img_size,int(nbSquaresWidth*img_size/2),X.shape[-1]))\n",
    "\n",
    "    inputs = [X[:1000], y[:1000] ]\n",
    "    #_, _, conditional_input = cvae.conditional_input(inputs)\n",
    "    reconstructedX = decoder.predict(encoder.predict(inputs)[2])\n",
    "\n",
    "    for i in range(nbSamples) :     # \n",
    "        #original = X[i]\n",
    "        reconstruction = reconstructedX[i]\n",
    "        rowIndex = i%nbSquaresWidth\n",
    "        columnIndex = int((i-rowIndex)/nbSquaresHeight)\n",
    "        resultImage[rowIndex*img_size:(rowIndex+1)*img_size,columnIndex*2*img_size:(columnIndex+1)*2*img_size,:] = np.hstack([reconstruction])\n",
    "\n",
    "    return resultImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions for samples in dataset\n",
    "def visualizeGeneratedImages(encoder, decoder, save=False):\n",
    "    X_healthy = generate_random_img(decoder, labels = (0,1) )\n",
    "    X_cancer = generate_random_img(decoder, labels= (1,0) )\n",
    "    healthyReconstruction = getGeneratedImages(X = X_healthy, y= (0,1), encoder = encoder, decoder = decoder)\n",
    "    cancerReconstruction = getGeneratedImages(X= X_cancer, y= (1,0), encoder = encoder, decoder = decoder)\n",
    "\n",
    "    if not save:\n",
    "        print(\"Generating 10 image reconstructions...\")\n",
    "\n",
    "    result = np.hstack([healthyReconstruction,\n",
    "            np.zeros([healthyReconstruction.shape[0],5,\n",
    "            healthyReconstruction.shape[-1]]),\n",
    "            cancerReconstruction])\n",
    "    result = (result*255.).astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        fig, _ = plt.subplots(figsize=(15, 15))\n",
    "        plt.imshow(result)\n",
    "        fig.savefig('img/Cvae_generated_epochs:{}_beta:{}.png'.format(epochs, beta_coeff))\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeGeneratedImages(vae_encoder, vae_decoder, save=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "dis_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
