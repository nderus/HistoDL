{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return np.array([i for i, elem in enumerate(lst) if condition(elem)])\n",
    "    \n",
    "def plot_2d_data_categorical(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  fig, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "  colors = np.array(['#7FFFD4', '#458B74', '#0000CD', '#EE3B3B', '#7AC5CD', '#66CD00',\n",
    "         '#EE7621', '#3D59AB', '#CD950C', '#483D8B'])\n",
    "  for i in range(len(data_2d)):\n",
    "      for k in range(category_count):\n",
    "\n",
    "        index = find_indices(y[i], lambda e: e == k)\n",
    "\n",
    "        data_2d_k = data_2d[i][index, ]\n",
    "        y_k = y[i][index]\n",
    "\n",
    "        if (titles != None):\n",
    "          axs[k,i].set_title(\"{} - Class: {}\".format(titles[i], k))\n",
    "\n",
    "        scatter = axs[k, i].scatter(data_2d_k[:, 0], data_2d_k[:, 1],\n",
    "                                s=1, c=colors[k], cmap=plt.cm.Paired)\n",
    "        axs[k, i].legend(*scatter.legend_elements())\n",
    "        axs[k, i].set_xlim([-3, 3])\n",
    "        axs[k, i].set_ylim([-3, 3])\n",
    "        wandb.log({\"Embdedding_classes\": wandb.Image(plt)})\n",
    "        fig.savefig('reports/' + directory + '/encoding_categorical')\n",
    "        \n",
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "    wandb.log({\"Embdedding\": wandb.Image(plt)})\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols, digit_label,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "  \n",
    "  wandb.log({\"Latent_interpolation_class: {}\".format(digit_label): wandb.Image(plt)})\n",
    "\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10):\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "    labels = ones * labels\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Val_Plot(loss, val_loss, reconstruction_loss, val_reconstruction_loss, kl_loss, val_kl_loss):\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,4, figsize= (16,4))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(loss) + 1), loss)\n",
    "    ax1.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax1.set_title('History of Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "    ax2.plot(range(1, len(reconstruction_loss) + 1), reconstruction_loss)\n",
    "    ax2.plot(range(1, len(val_reconstruction_loss) + 1), val_reconstruction_loss)\n",
    "    ax2.set_title('History of reconstruction_loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('reconstruction_loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "    \n",
    "    ax3.plot(range(1, len(kl_loss) + 1), kl_loss)\n",
    "    ax3.plot(range(1, len(val_kl_loss) + 1), val_kl_loss)\n",
    "    ax3.set_title(' History of kl_loss')\n",
    "    ax3.set_xlabel(' Epochs ')\n",
    "    ax3.set_ylabel('kl_loss')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "    wandb.log({\"Training\": wandb.Image(plt)})\n",
    "    plt.show()\n",
    "    fig.savefig('reports/' + directory + '/loss_function')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data flatten shape:  (60000, 28, 28)\n",
      "Train label shape:  (60000,)\n",
      "Test data flatten shape:  (10000, 28, 28)\n",
      "Test label shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"mnist\"\n",
    "category_count=10 \n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Test data flatten shape: ',test_x.shape)\n",
    "print('Test label shape: ',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "         \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data flatten shape:  (50000, 28, 28)\n",
      "Train label shape:  (50000,)\n",
      "Validation data flatten shape:  (10000, 28, 28)\n",
      "Validation label shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "val_size=10000\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y,\n",
    "                            test_size = val_size,random_state = 1,shuffle=True)\n",
    "\n",
    "print('Train data flatten shape: ',train_x.shape)\n",
    "print('Train label shape: ',train_y.shape)\n",
    "print('Validation data flatten shape: ',val_x.shape)\n",
    "print('Validation label shape: ',val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (50000, 28, 28, 1)\n",
      "Validation shape:  (10000, 28, 28, 1)\n",
      "Test shape:  (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "if len(train_x.shape) == 3:\n",
    "    train_x=np.expand_dims(train_x,axis=3)\n",
    "    val_x=np.expand_dims(val_x,axis=3)\n",
    "    test_x=np.expand_dims(test_x,axis=3)\n",
    "    print('Train shape: ',train_x.shape)\n",
    "    print('Validation shape: ',val_x.shape)\n",
    "    print('Test shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value:  0.0\n",
      "Max value:  1.0\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x/255.0\n",
    "val_x = val_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "print('Max value: ', train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data flatten shape:  (50000, 784)\n",
      "Validation data flatten shape:  (10000, 784)\n",
      "Test data flatten shape:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "original_image_shape=(train_x.shape[1], train_x.shape[2])\n",
    "\n",
    "train_x_flatten=np.reshape(train_x,(train_x.shape[0],-1))\n",
    "val_x_flatten=np.reshape(val_x,(val_x.shape[0],-1))\n",
    "test_x_flatten=np.reshape(test_x,(test_x.shape[0],-1))\n",
    "\n",
    "print('Train data flatten shape: ',train_x_flatten.shape)\n",
    "print('Validation data flatten shape: ',val_x_flatten.shape)\n",
    "print('Test data flatten shape: ',test_x_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label one hot encoding shape:  (50000, 10)\n",
      "Validation label one hot encoding shape:  (10000, 10)\n",
      "Test label one hot encoding shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_y_one_hot = to_categorical(train_y,category_count)\n",
    "val_y_one_hot=to_categorical(val_y,category_count)\n",
    "test_y_one_hot=to_categorical(test_y,category_count)\n",
    "\n",
    "print('Train label one hot encoding shape: ',train_y_one_hot.shape)\n",
    "print('Validation label one hot encoding shape: ',val_y_one_hot.shape)\n",
    "print('Test label one hot encoding shape: ',test_y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 50, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 50, 50, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 25, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 25, 25, 128)      73856     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 50, 50, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50, 50, 10)        1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,450\n",
      "Trainable params: 261,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 01:44:25.126422: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input((50, 50, 3))\n",
    "\n",
    "x = layers.Conv2D(64, 3, 1, padding='same')(inputs)\n",
    "\n",
    "x = layers.Conv2D(64, 3, 2, padding='same')(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, 3, 1, padding='same')(x)\n",
    "x = layers.Conv2DTranspose(128, 3, 2, padding='same')(x)\n",
    "\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu brings a lot of activation values = 0, leaky seems better\n",
    "# https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24\n",
    "\n",
    "def bn_relu(inputs):\n",
    "    bn = layers.BatchNormalization()(inputs)\n",
    "    relu = layers.LeakyReLU(0.2)(bn)\n",
    "    return(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderResBlock(keras.Model):\n",
    "    def __init__(self, filters, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = layers.Conv2D(filters, 3, 2, padding='same')\n",
    "            self.shortcut = keras.Sequential([\n",
    "                layers.Conv2D(filters, 1, 2),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        else:\n",
    "            self.conv1 = layers.Conv2D(filters, 3, 1, padding='same')\n",
    "            self.shortcut = keras.Sequential()\n",
    " \n",
    "        self.conv2 = layers.Conv2D(filters, 3, 1, padding='same')\n",
    "    def call(self, input):\n",
    "        shortcut = self.shortcut(input)\n",
    "\n",
    "        input = self.conv1(input)\n",
    "        input = layers.BatchNormalization()(input)\n",
    "        input = layers.ReLU()(input)\n",
    "\n",
    "        input = self.conv2(input)\n",
    "        input = layers.BatchNormalization()(input)\n",
    "        input = layers.ReLU()(input)\n",
    "\n",
    "        input = input + shortcut\n",
    "        return layers.ReLU()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderResNet(keras.Model):\n",
    "    def __init__(self, resblock, repeat, encoded_dim):\n",
    "        super().__init__()\n",
    "        self.layer0 = keras.Sequential([\n",
    "            layers.Conv2D(64, 7, 2, padding='same'),\n",
    "            layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ], name='layer0')\n",
    "\n",
    "        self.layer1 = keras.Sequential([\n",
    "            resblock(64, downsample=False) for _ in range(repeat[0])\n",
    "        ], name='layer1')\n",
    "\n",
    "        self.layer2 = keras.Sequential([\n",
    "            resblock(128, downsample=True)\n",
    "        ] + [\n",
    "            resblock(128, downsample=False) for _ in range(1, repeat[1])\n",
    "        ], name='layer2')\n",
    "\n",
    "        self.layer3 = keras.Sequential([\n",
    "            resblock(256, downsample=True)\n",
    "        ] + [\n",
    "            resblock(256, downsample=False) for _ in range(1, repeat[2])\n",
    "        ], name='layer3')\n",
    "\n",
    "        self.layer4 = keras.Sequential([\n",
    "            resblock(512, downsample=True)\n",
    "        ] + [\n",
    "            resblock(512, downsample=False) for _ in range(1, repeat[3])\n",
    "        ], name='layer4')\n",
    "\n",
    "        self.flat = layers.Flatten(name = 'flatten')\n",
    "        self.bottleneck = layers.Dense(encoded_dim * 2, name='encoder_bottleneck')\n",
    "        self.mu = layers.Dense(encoded_dim, name='mu')\n",
    "        self.log_var = layers.Dense(encoded_dim, name='log_var')\n",
    "\n",
    "    def call(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        input = self.layer4(input)\n",
    "        input = self.flat(input)\n",
    "        input = self.bottleneck(input)\n",
    "        mu = self.mu(input)\n",
    "        log_var = self.log_var(input)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderResNet18(EncoderResNet):\n",
    "    def __init__(self, encoded_dim):\n",
    "        super().__init__(EncoderResBlock, [2, 2, 2, 2], encoded_dim)\n",
    "\n",
    "    def call(self, input):\n",
    "        return super().call(input)\n",
    "\n",
    "    def model(self, input_shape):\n",
    "        x = keras.Input(input_shape, name='input')\n",
    "        return keras.models.Model(x, self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 50, 50, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " layer0 (Sequential)            (None, 13, 13, 64)   9728        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " layer1 (Sequential)            (None, 13, 13, 64)   147712      ['layer0[0][0]']                 \n",
      "                                                                                                  \n",
      " layer2 (Sequential)            (None, 7, 7, 128)    525440      ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer3 (Sequential)            (None, 4, 4, 256)    2099456     ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer4 (Sequential)            (None, 2, 2, 512)    8393216     ['layer3[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['layer4[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder_bottleneck (Dense)     (None, 256)          524544      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          32896       ['encoder_bottleneck[0][0]']     \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 128)          32896       ['encoder_bottleneck[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,765,888\n",
      "Trainable params: 11,763,968\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder, ok.\n",
    "resnet18 = EncoderResNet18(encoded_dim = 128)\n",
    "resnet18 = resnet18.model(input_shape=(50, 50, 3))\n",
    "resnet18.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderResBlock(keras.Model):\n",
    "    def __init__(self, filters, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        if upsample:\n",
    "            self.conv1 = layers.Conv2DTranspose(filters, 3, 2, padding='same')\n",
    "            self.shortcut = keras.Sequential([\n",
    "                layers.Conv2DTranspose(filters, 1, 2),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        else:\n",
    "            self.conv1 = layers.Conv2DTranspose(filters, 3, 1, padding='same')\n",
    "            self.shortcut = keras.Sequential()\n",
    " \n",
    "        self.conv2 = layers.Conv2DTranspose(filters, 3, 1, padding='same')\n",
    "    def call(self, inputs):\n",
    "    \n",
    "        shortcut = self.shortcut(inputs)\n",
    "\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = layers.BatchNormalization()(inputs)\n",
    "        inputs = layers.ReLU()(inputs)\n",
    "\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = layers.BatchNormalization()(inputs)\n",
    "        inputs = layers.ReLU()(inputs)\n",
    "\n",
    "        inputs = inputs + shortcut\n",
    "        return layers.ReLU()(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecoderResBlock at 0x7f85902d2a00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecoderResBlock(6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderResNet(keras.Model):\n",
    "    def __init__(self, resblock, repeat, encoded_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer5 = keras.Sequential([\n",
    "            resblock(512, upsample=False)\n",
    "        ] + [\n",
    "            resblock(512, upsample=False)  for _ in range(1, repeat[0])\n",
    "        ], name='layer5')\n",
    "\n",
    "\n",
    "        self.layer6 = keras.Sequential([\n",
    "            resblock(256, upsample=True)\n",
    "        ] + [\n",
    "            resblock(256, upsample=False) for _ in range(1, repeat[1])\n",
    "        ], name='layer6')\n",
    "\n",
    "\n",
    "        self.layer7 = keras.Sequential([\n",
    "            resblock(128, upsample=True)\n",
    "        ] + [\n",
    "            resblock(128, upsample=False) for _ in range(1, repeat[2])\n",
    "        ], name='layer7')\n",
    "        \n",
    "        # self.layer7 = keras.Sequential([ # TO DO: change back this into resblock, heigth/depth issue\n",
    "        #     layers.Conv2DTranspose(128, 4, 1, padding='valid'),\n",
    "        #     #layers.BatchNormalization(),\n",
    "        #     layers.ReLU()\n",
    "        # ], name='layer7')\n",
    "\n",
    "        self.layer8 =  keras.Sequential([ \n",
    "            resblock(64, upsample=True)\n",
    "        ] + [\n",
    "            resblock(64, upsample=False) for _ in range(repeat[3])\n",
    "        ], name='layer8')\n",
    "\n",
    "        self.layer9 = keras.Sequential([\n",
    "                layers.Conv2DTranspose(64, 7, 1, padding='same'),\n",
    "                layers.UpSampling2D(3),\n",
    "                #layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU()\n",
    "            ], name='layer9')\n",
    "          \n",
    "        self.bottleneck = layers.Dense(encoded_dim * 2, name='bottleneck')\n",
    "        self.pre_reshape = layers.Dense(2*2*512, name='pre_reshape')\n",
    "        self.reshape = layers.Reshape(target_shape=(2, 2, 512), name = 'reshape')\n",
    "        self.output_layer = layers.Conv2DTranspose(filters = 3, kernel_size=3, strides=1, activation='sigmoid' ,padding='valid', name='outputs')\n",
    "\n",
    "    def call(self, input):\n",
    "        input = self.bottleneck(input)\n",
    "        input = self.pre_reshape(input)\n",
    "        input = self.reshape(input)\n",
    "        input = self.layer5(input)\n",
    "        input = self.layer6(input)\n",
    "        input = self.layer7(input)\n",
    "        input = self.layer8(input)\n",
    "        input = self.layer9(input)\n",
    "        out = self.output_layer(input)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderResNet18(DecoderResNet):\n",
    "    def __init__(self, encoded_dim):\n",
    "        super().__init__(DecoderResBlock, [2, 2, 2, 1], encoded_dim)\n",
    "\n",
    "    def call(self, input):\n",
    "        return super().call(input)\n",
    "\n",
    "    def model(self, input_shape):\n",
    "        x = keras.Input(input_shape, name='input')\n",
    "        return keras.models.Model(x, self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 138)]             0         \n",
      "                                                                 \n",
      " bottleneck (Dense)          (None, 256)               35584     \n",
      "                                                                 \n",
      " pre_reshape (Dense)         (None, 2048)              526336    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " layer5 (Sequential)         (None, 2, 2, 512)         9439232   \n",
      "                                                                 \n",
      " layer6 (Sequential)         (None, 4, 4, 256)         3082496   \n",
      "                                                                 \n",
      " layer7 (Sequential)         (None, 8, 8, 128)         771200    \n",
      "                                                                 \n",
      " layer8 (Sequential)         (None, 16, 16, 64)        193088    \n",
      "                                                                 \n",
      " layer9 (Sequential)         (None, 48, 48, 64)        201024    \n",
      "                                                                 \n",
      " outputs (Conv2DTranspose)   (None, 50, 50, 3)         1731      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,250,691\n",
      "Trainable params: 14,249,667\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#decoder, ok?\n",
    "decoder = DecoderResNet18( encoded_dim = 128)\n",
    "decoder = decoder.model(input_shape=(encoded_dim + category_count,))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_encoder = resnet18\n",
    "cvae_decoder = decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 50, 50, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " layer0 (Sequential)            (None, 13, 13, 64)   9728        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " layer1 (Sequential)            (None, 13, 13, 64)   147712      ['layer0[0][0]']                 \n",
      "                                                                                                  \n",
      " layer2 (Sequential)            (None, 7, 7, 128)    525440      ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer3 (Sequential)            (None, 4, 4, 256)    2099456     ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer4 (Sequential)            (None, 2, 2, 512)    8393216     ['layer3[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['layer4[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder_bottleneck (Dense)     (None, 256)          524544      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          32896       ['encoder_bottleneck[0][0]']     \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 128)          32896       ['encoder_bottleneck[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,765,888\n",
      "Trainable params: 11,763,968\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 138)]             0         \n",
      "                                                                 \n",
      " bottleneck (Dense)          (None, 256)               35584     \n",
      "                                                                 \n",
      " pre_reshape (Dense)         (None, 2048)              526336    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " layer5 (Sequential)         (None, 2, 2, 512)         9439232   \n",
      "                                                                 \n",
      " layer6 (Sequential)         (None, 4, 4, 256)         3082496   \n",
      "                                                                 \n",
      " layer7 (Sequential)         (None, 8, 8, 128)         771200    \n",
      "                                                                 \n",
      " layer8 (Sequential)         (None, 16, 16, 64)        193088    \n",
      "                                                                 \n",
      " layer9 (Sequential)         (None, 48, 48, 64)        201024    \n",
      "                                                                 \n",
      " outputs (Conv2DTranspose)   (None, 50, 50, 3)         1731      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,250,691\n",
      "Trainable params: 14,249,667\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=574x303>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(cvae_encoder, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=606x303>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(cvae_decoder, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, shape, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.shape = shape\n",
    "        self.latent_var = []\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.total_loss_no_weights_tracker = keras.metrics.Mean(name=\"loss_no_weights\")\n",
    "        #\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"val_reconstruction_loss\")\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "        self.val_total_loss_no_weights_tracker = keras.metrics.Mean(name=\"val_loss_no_weights\")\n",
    "        self.latent_var_tracker = keras.metrics.Mean(name=\"latent_var\") #added\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.val_total_loss_tracker,\n",
    "            self.val_reconstruction_loss_tracker,\n",
    "            self.val_kl_loss_tracker,\n",
    "            self.total_loss_no_weights_tracker,\n",
    "            self.val_total_loss_no_weights_tracker,\n",
    "            self.latent_var_tracker,\n",
    "\n",
    "        ]\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "\n",
    "  \n",
    "    def conditional_input(self, inputs, label_size=10):\n",
    "        image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                    dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "        labels = ones * labels\n",
    "\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "    def sampling(self, z_mean, z_log_var, input_label):\n",
    "        if len(input_label.shape) == 1:\n",
    "            input_label = np.expand_dims(input_label, axis=0)\n",
    "\n",
    "        eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                               mean=0., stddev=1.0, name='epsilon')\n",
    "        z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1)\n",
    "        return z_cond\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(data[1].shape)\n",
    "            input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "            batch_size = input_img.shape[0]\n",
    "            print('batch_size', batch_size)\n",
    "            z_mean, z_log_var = self.encoder(conditional_input)\n",
    "            z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "\n",
    "            #reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "            #                        tf.keras.backend.flatten(reconstruction))\n",
    "            reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img, # removed np.prod(self.shape) *\n",
    "                                    reconstruction), axis=(1, 2))            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                      - tf.exp(z_log_var))\n",
    "\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1) #sum over encoded dimensiosn, average over batch\n",
    "            total_loss_no_weights = reconstruction_loss + kl_loss\n",
    "            total_loss_no_weights = tf.reduce_mean(total_loss_no_weights)\n",
    "            kl_loss = self.beta * kl_loss\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            total_loss = tf.reduce_mean(total_loss) \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_no_weights_tracker.update_state(total_loss_no_weights)\n",
    "        self.latent_var_tracker.update_state(z_log_var)\n",
    "        #wandb.log({\"loss\": total_loss, \"reconstructon_loss\": reconstruction_loss, \"kl_loss\": kl_loss,})\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"loss_no_weights\": self.total_loss_no_weights_tracker.result(),\n",
    "            \"latent_var\": self.latent_var_tracker.result()\n",
    "        }\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        \n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))   \n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                  - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        total_loss_no_weights = reconstruction_loss + kl_loss\n",
    "        total_loss_no_weights = tf.reduce_mean(total_loss_no_weights)\n",
    "        kl_loss = self.beta * kl_loss\n",
    "        total_loss =  reconstruction_loss + kl_loss\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_no_weights_tracker.update_state(total_loss_no_weights)\n",
    "        return{\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "            \"loss_no_weights\": self.total_loss_no_weights_tracker.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl coefficient: 1.000\n",
      "Model: \"cvae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_1 (Functional)        [(None, 128),             11765888  \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 50, 50, 3)         14250691  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,016,597\n",
      "Trainable params: 26,013,635\n",
      "Non-trainable params: 2,962\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#kl_coefficient = encoded_dim / (input_shape[0] * input_shape[1] * input_shape[2])\n",
    "kl_coefficient = 1\n",
    "print('kl coefficient: {:.3f}'.format(kl_coefficient))\n",
    "# from b vae paper, use beta = encoded_dimension / pixel_dimension i.e. -> 0.068\n",
    "cvae = CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)\n",
    "cvae.built = True\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10): \n",
    "    print('ones_dim', inputs[0].shape[0])\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                    dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    print('image_size:', image_size)\n",
    "\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    print('labels dim:', labels.shape)\n",
    "\n",
    "    ones = tf.ones([batch_size] + image_size[0:-1] + [label_size]) \n",
    "    labels = ones * labels\n",
    "    print('labels dim:', labels.shape)\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_input = cvae.encoder.input[0]\n",
    "cvae_output = cvae.decoder.output\n",
    "mu = cvae.encoder.get_layer('mu').output\n",
    "log_var = cvae.encoder.get_layer('log_var').output\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "cvae.compile(optimizer = opt)\n",
    "#cvae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_img: (32, 28, 28, 1)\n",
      "input_label: (32, 10)\n",
      "labels_1: (32, 1, 1, 10)\n",
      "labels_2: (32, 1, 1, 10)\n",
      "ones: (32, 28, 28, 10)\n",
      "labels_ones: (32, 28, 28, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 28, 28, 11])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [train_x[:32], train_y_one_hot[:32]]\n",
    "image_size = [28, 28, 1]\n",
    "label_size = 10\n",
    "\n",
    "input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "print('input_img:', input_img.shape)\n",
    "input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                dtype ='float32')(inputs[1])\n",
    "print('input_label:', input_label.shape)\n",
    "\n",
    "labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "print('labels_1:', labels.shape)\n",
    "\n",
    "labels = tf.cast(labels, dtype='float32')\n",
    "print('labels_2:', labels.shape)\n",
    "\n",
    "ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "print('ones:', ones.shape)\n",
    "labels = ones * labels\n",
    "#labels = labels[:, :, :, :1]\n",
    "print('labels_ones:', labels.shape)\n",
    "\n",
    "conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "conditional_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(28, 28), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(conditional_input[:1, :, :, 9:10], [28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(0, 10, size=(3, 3, 1))\n",
    "B= np.random.randint(0, 10, size=(1, 1, 10))\n",
    "#print(A)\n",
    "#print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C = A * B\n",
    "#print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                1640      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3072)              125952    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 127,592\n",
      "Trainable params: 127,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(40,))\n",
    "layer1 = layers.Dense(40)(input)\n",
    "layer2 = layers.Dense( 32 * 32* 3)(layer1)\n",
    "a = keras.Model(input, layer2)\n",
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean = np.random.normal(size=(100, 512))\n",
    "z_log_var =  np.random.normal(size=(100, 512))\n",
    "\n",
    "kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "            - tf.exp(z_log_var))\n",
    "\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kl_loss = tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "            - tf.exp(z_log_var)), axis=1) #sum over encoded dimensions\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "#kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=614645.561131235>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = np.random.normal(size=(100, 32, 32, 3))\n",
    "reconstruction = np.random.normal(size=(100, 32, 32, 3))\n",
    "\n",
    "reconstruction_loss = np.prod(input_img.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "                                    tf.keras.backend.flatten(reconstruction)) \n",
    "\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614645.561131235"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(tf.Variable(reconstruction_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reconstruction_loss = tf.reduce_sum(\n",
    "\n",
    "                keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))\n",
    "\n",
    "reconstruction_loss.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = kl_coefficient * kl_loss\n",
    "total_loss =  reconstruction_loss + kl_loss\n",
    "total_loss = tf.reduce_mean(total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '20220707-014832_cvae_mnist_ndim:128_kl:1.000_epoch:100_batch:100' created\n"
     ]
    }
   ],
   "source": [
    "# Directory\n",
    "directory = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_{}_{}_ndim:{}_kl:{:.3f}_epoch:{}_batch:{}\".format(cvae.name, \n",
    "             dataset_name, encoded_dim, kl_coefficient, epoch_count, batch_size)\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"reports\"\n",
    "  \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "  \n",
    "# Create the directory\n",
    "# 'GeeksForGeeks' in\n",
    "# '/home / User / Documents'\n",
    "os.mkdir(path)\n",
    "print(\"Directory '%s' created\" %directory)\n",
    "\n",
    "os.mkdir(path + '/activations')\n",
    "os.mkdir(path + '/filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "#wandb.init(project=\"my-test-project\", entity=\"nrderus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnrderus\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/PERSONALE/nicolas.derus2/HistoDL/wandb/run-20220707_014834-1tlw6qzr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nrderus/HistoDL/runs/1tlw6qzr\" target=\"_blank\">glad-butterfly-366</a></strong> to <a href=\"https://wandb.ai/nrderus/HistoDL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nrderus/HistoDL/runs/1tlw6qzr?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f86f0b76a90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience = 5\n",
    "\n",
    "\n",
    "wandb.init(project=\"HistoDL\", entity=\"nrderus\",\n",
    "  config = {\n",
    "  \"dataset\": dataset_name,\n",
    "  \"model\": \"CVAE\",\n",
    "  \"encoded_dim\": encoded_dim,\n",
    "  \"kl_coefficient\": kl_coefficient,\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epoch_count,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"patience\": patience,\n",
    "  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "(100, 10)\n",
      "batch_size 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_24104/650161473.py\", line 78, in train_step\n        z_mean, z_log_var = self.encoder(conditional_input)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 50, 50, 3), found shape=(100, 28, 28, 11)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb Cell 69'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=0'>1</a>\u001b[0m early_stop \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=1'>2</a>\u001b[0m              patience\u001b[39m=\u001b[39mpatience, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=3'>4</a>\u001b[0m history \u001b[39m=\u001b[39m cvae\u001b[39m.\u001b[39;49mfit([train_x,train_y_one_hot],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=4'>5</a>\u001b[0m                    validation_data\u001b[39m=\u001b[39;49m([val_x,val_y_one_hot],\u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=5'>6</a>\u001b[0m                    epochs\u001b[39m=\u001b[39;49mepoch_count,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=6'>7</a>\u001b[0m                    batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy2_resnet.ipynb#ch0000066vscode-remote?line=7'>8</a>\u001b[0m                    callbacks\u001b[39m=\u001b[39;49m[early_stop, WandbCallback(save_weights_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  ])\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/wandb/integration/keras/keras.py:163\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    162\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_24104/650161473.py\", line 78, in train_step\n        z_mean, z_log_var = self.encoder(conditional_input)\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 50, 50, 3), found shape=(100, 28, 28, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "history = cvae.fit([train_x,train_y_one_hot],\n",
    "                   validation_data=([val_x,val_y_one_hot],None),\n",
    "                   epochs=epoch_count,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[early_stop, WandbCallback(save_weights_only=True)  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.get_value(cvae.latent_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.numpy_function(cvae.latent_var[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Val_Plot(history.history['loss'][1:],\n",
    "#                history.history['val_loss'][1:],\n",
    "#                history.history['reconstruction_loss'][1:],\n",
    "               \n",
    "#                history.history['val_reconstruction_loss'][1:],\n",
    "#                history.history['kl_loss'][1:],\n",
    "#                history.history['val_kl_loss'][1:]\n",
    "#                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward the port 6006 on server on 12006 on  my machine\n",
    "# ssh -N -L 16006:127.0.0.1:6006 nicolas.derus2@137.204.48.211\n",
    "# access with http://127.0.0.1:16006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cvae.evaluate([test_x, test_y_one_hot],None,\n",
    "            batch_size=batch_size,verbose=0)\n",
    "print('Test loss: {:.3f}'.format(test_loss[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_size = 10\n",
    "_, input_label_train, train_input = cvae.conditional_input([train_x, train_y_one_hot])\n",
    "_, input_label_test, test_input = cvae.conditional_input([test_x, test_y_one_hot])\n",
    "_, input_label_val, val_input = cvae.conditional_input([val_x, val_y_one_hot])\n",
    "\n",
    "\n",
    "print(input_label_train.shape)\n",
    "print(train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict(train_input)\n",
    "test_x_mean, test_log_var = cvae.encoder.predict(test_input)\n",
    "val_x_mean, val_log_var = cvae.encoder.predict(val_input)\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim > 2:\n",
    "    from sklearn import manifold\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    train_x_tsne = tsne.fit_transform(train_x_mean[:2000])\n",
    "    test_x_tsne = tsne.fit_transform(test_x_mean[:2000])\n",
    "    val_x_tsne = tsne.fit_transform(val_x_mean[:2000])\n",
    "    plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:2000], test_y[:2000] ,val_y[:2000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "            [train_y, test_y ,val_y],\n",
    "            ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "    plot_2d_data( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, input_label_train)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, input_label_test)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, input_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 10\n",
    "\n",
    "fig, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  #axs[0, i].set_title(train_y[random_idx])\n",
    "  axs[0, i].set_title( labels[int(train_y[random_idx])]  )\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "wandb.log({\"Reconstructions\": wandb.Image(plt)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cvae.latent_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim),\n",
    "                             mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 3\n",
    "digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cond = reparametrization(z_mean=0, z_log_var=0.2, input_label = b)\n",
    "decoded_x = cvae_decoder.predict(z_cond)\n",
    "digit = decoded_x[0].reshape(input_shape) \n",
    "plt.axis('off')\n",
    "plt.imshow(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label = 2\n",
    "_, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "    digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "    a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "    b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "    z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    digit_0 = decoded_x[0].reshape(input_shape) \n",
    "    digit_1 = decoded_x[1].reshape(input_shape) \n",
    "    axs[0, i].imshow(digit_0)\n",
    "    axs[0, i].axis('off')\n",
    "    #axs[0, i].set_title(digit_label)\n",
    "    axs[0, i].set_title( labels[digit_label]  )\n",
    "    axs[1, i].imshow(digit_1)\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title( labels[digit_label]  )\n",
    "wandb.log({\"Generations\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generations_class(digit_label=1):\n",
    "    _, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "    for i in range(image_count):\n",
    "        digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "        a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "        b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "        z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "        decoded_x = cvae_decoder.predict(z_cond)\n",
    "        digit_0 = decoded_x[0].reshape(input_shape) \n",
    "        digit_1 = decoded_x[1].reshape(input_shape) \n",
    "        axs[0, i].imshow(digit_0)\n",
    "        axs[0, i].axis('off')\n",
    "        #axs[0, i].set_title(digit_label)\n",
    "        axs[0, i].set_title( labels[digit_label]  )\n",
    "        axs[1, i].imshow(digit_1)\n",
    "        axs[1, i].axis('off')\n",
    "        axs[1, i].set_title( labels[digit_label]  )\n",
    "        wandb.log({\"Generations: {}\".format(digit_label): wandb.Image(plt)})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = category_count\n",
    "if (label_size <= 10):\n",
    "    for i in range(label_size):\n",
    "        generations_class(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.save_weights('weights/cvae_toy.h5')\n",
    "cvae_encoder.save('models/cvae_encoder_toy.h5')\n",
    "cvae_decoder.save('models/cvae_decoder_toy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_interpolation(digit_label=1):\n",
    "  n = 10 # number of images per row and column\n",
    "  limit=3 # random values are sampled from the range [-limit,+limit]\n",
    "  digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "  a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "  grid_x = np.linspace(-limit,limit, n) \n",
    "  grid_y = np.linspace(limit,-limit, n)\n",
    "\n",
    "  generated_images=[]\n",
    "  for i, yi in enumerate(grid_y):\n",
    "    single_row_generated_images=[]\n",
    "    for j, xi in enumerate(grid_x):\n",
    "      random_sample = np.array([[xi, yi]])\n",
    "      digit_label_one_hot = to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "      a = tf.convert_to_tensor(digit_label_one_hot)\n",
    "      b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "      z_cond = reparametrization(z_mean=random_sample, z_log_var=0.0, input_label = b)\n",
    "      decoded_x = cvae.decoder.predict(z_cond)\n",
    "      single_row_generated_images.append(decoded_x[0].reshape(original_image_shape))\n",
    "    generated_images.append(single_row_generated_images)      \n",
    "  plot_generated_images(generated_images,n,n,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = category_count\n",
    "if (encoded_dim == 2 & label_size <= 10):\n",
    "    for i in range(label_size):\n",
    "        latent_space_interpolation(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvae.built = True\n",
    "#cvae.load_weights('weights/vae_toy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True) # TEMPORARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model):\n",
    "    test = test_x[1]\n",
    "    plt.imshow(test)\n",
    "    #test = image.img_to_array(test)\n",
    "    test = np.expand_dims(test, axis=0)\n",
    "    test.shape\n",
    "    test_label = test_y_one_hot[0]\n",
    "    img_tensor = [test, test_label]\n",
    "    from keras import models\n",
    "\n",
    "    # Extracts the outputs of the top 8 layers:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    layer_outputs = []\n",
    "    layer_names = []\n",
    "    for layer in model.layers[1:]:\n",
    "        \n",
    "        try: \n",
    "            layer_outputs.append(layer.get_output_at(1))\n",
    "            layer_names.append(layer.name)\n",
    "        \n",
    "        except:\n",
    "            layer_outputs.append(layer.output)\n",
    "            layer_names.append(layer.name)\n",
    "\n",
    "    # Creates a model that will return these outputs, given the model input:\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # This will return a list of 5 Numpy arrays:\n",
    "    # one array per layer activation\n",
    "    if 'encoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        activations = activation_model.predict(conditional_input) #for encoder\n",
    "\n",
    "    if 'decoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        input_label = np.expand_dims(input_label, axis=0)\n",
    "        z_mean, z_log_var = cvae.encoder(conditional_input)\n",
    "        z_cond = cvae.sampling(z_mean, z_log_var, input_label)\n",
    "        \n",
    "        activations = activation_model.predict(z_cond) #for decoder\n",
    "    \n",
    "    for activation, name in zip(activations[0:], layer_names[0:]):\n",
    "        print(name)\n",
    "        print(activation.shape)\n",
    "    \n",
    "    for counter, (activation, name) in enumerate(zip(activations[0:], layer_names[0:])):\n",
    "        print(name)\n",
    "        plot_filters(activation, name, counter, model_name=model.name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_x[1]\n",
    "plt.imshow(test)\n",
    "#test = image.img_to_array(test)\n",
    "test = np.expand_dims(test, axis=0)\n",
    "test.shape\n",
    "test_label = test_y_one_hot[0]\n",
    "img_tensor = [test, test_label]\n",
    "input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_img.shape)\n",
    "print(input_label.shape)\n",
    "print(conditional_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [input_img, input_label]\n",
    "image_size = [28, 28, 1]\n",
    "input_img = layers.InputLayer(input_shape=image_size,\n",
    "                            dtype ='float32')(inputs[0])\n",
    "input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                dtype ='float32')(inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "print(labels)\n",
    "labels = tf.cast(labels, dtype='float32')\n",
    "print(labels)\n",
    "ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "print(ones.shape)\n",
    "labels = ones * labels \n",
    "print(labels)\n",
    "conditional_input = layers.Concatenate(axis=3)([input_img, labels]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_filters(activation_layer, layer_name, counter, model_name):\n",
    "    if len(activation_layer.shape) == 2: # if flat layer\n",
    "        print('flat')\n",
    "        return None\n",
    "        if activation_layer.shape[1] == 1875:\n",
    "            activation_layer = activation_layer.reshape(1, 25, 25, 3)\n",
    "        if activation_layer.shape[1] == 1024:\n",
    "           activation_layer = activation_layer.reshape(1, 16, 16, 4)\n",
    "        if activation_layer.shape[1] == 512:\n",
    "           activation_layer = activation_layer.reshape(1, 8, 8, 8)\n",
    "\n",
    "    n = math.floor(np.sqrt(activation_layer.shape[3]))\n",
    "\n",
    "    if int(n + 0.5) ** 2 == activation_layer.shape[3]:\n",
    "\n",
    "        m = n\n",
    "    else:\n",
    "        m = math.floor(activation_layer.shape[3] / n)\n",
    "\n",
    "    if activation_layer.shape[3] == 1:\n",
    "        fig, ax = plt.subplots(1, 1, sharex='col', sharey='row',\n",
    "                                figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "\n",
    "        ax.imshow(activation_layer[0,:, :, 0], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "        fig.savefig(\"reports/\" + directory + '/activations/{}_{}_activations_{}.png'.format(model_name,\n",
    "                     counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "            \n",
    "    if n == 1:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, sharex='col', sharey='row',figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(activation_layer[0,:, :, i], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "        fig.savefig(\"reports/\" +directory +'/activations/{}_{}_activations_{}.png'.format(model_name, counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "    fig, ax = plt.subplots(n, m, sharex='col', sharey='row',figsize=(15, 15))\n",
    "    fig.suptitle(layer_name)\n",
    "    \n",
    " \n",
    "\n",
    "    filter_counter = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(activation_layer[0,:, :, filter_counter], cmap='viridis')\n",
    "            filter_counter += 1\n",
    "            if filter_counter == (activation_layer.shape[3] ):\n",
    "                break\n",
    "\n",
    "    wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "    fig.savefig(\"reports/\" + directory + \"/activations/{}_{}_activations_{}.png\".format(model_name, counter, layer_name))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations( cvae.encoder)\n",
    "visualize_activations(cvae.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cvae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    #img = img[ 25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conditional_input( inputs, label_size=10): \n",
    "  \n",
    "        image_size = [input_shape[0], input_shape[1], input_shape[2]]\n",
    "\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                        dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nth_filter_loss(filter_index, layer_name):\n",
    "    \"\"\"\n",
    "    We build a loss function that maximizes the activation\n",
    "    of the nth filter of the layer considered\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a connection between the input and the target layer\n",
    "    \n",
    "    submodel = tf.keras.models.Model([model.inputs[0]],\n",
    "                                     [model.get_layer(layer_name).output])\n",
    "\n",
    "# Initiate random noise\n",
    "\n",
    "    input_img_data = np.random.random((1, input_shape[0], input_shape[1],\n",
    "                                     input_shape[2]))\n",
    "\n",
    "    input_img_data =(input_img_data - 0.5) * 0.25\n",
    "    # Cast random noise from np.float64 to tf.float32 Variable\n",
    "    input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "\n",
    "    data = [input_img_data, train_y_one_hot[0]]\n",
    "    _, _, conditional_input_img = filter_conditional_input(data) \n",
    "    conditional_input_img= tf.Variable(tf.cast(conditional_input_img,\n",
    "                                         tf.float32))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = submodel(conditional_input_img)\n",
    "            loss_value = tf.reduce_mean(outputs[:, 2:-2, 2:-2, filter_index]) #removed borders in loss\n",
    "        grads = tape.gradient(loss_value, conditional_input_img)\n",
    "        normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads)))\n",
    "                                   + 1e-5)\n",
    "        #normalized_grads = tf.math.l2_normalize(grads)\n",
    "        conditional_input_img.assign_add(normalized_grads * step_size)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    #iterate = K.function([input_img], [loss_value, grads])\n",
    "\n",
    "    if loss_value > 0:\n",
    "        \n",
    "        #img = conditional_input_img.numpy().astype(np.float64)\n",
    "        #img = img.squeeze()\n",
    "        #img = deprocess_image(img) / 255.\n",
    "        img = conditional_input_img.numpy().astype(np.float64)\n",
    "        img = img.squeeze()\n",
    "        img = deprocess_image(img)\n",
    "        kept_filters.append((img, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the generated pictures for each filter.\n",
    "img_width = input_shape[0]\n",
    "img_height = input_shape[1]\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "print(input_img.shape)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "#layer_dict = dict([(layer.name, layer) for layer in model.layers[0:]])\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_filters = [layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "step_size = 10.\n",
    "kept_filters = []\n",
    "filters_dict = dict()\n",
    "for layer_name in layers_filters:\n",
    "    if 'conv' in layer_name:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        print('Processing filter for layer:', layer_name)\n",
    "        for filter_index in range(min(layer.output.shape[-1], 100)):\n",
    "            # print('Processing filter %d' % filter_index)\n",
    "            build_nth_filter_loss(filter_index, layer_name)\n",
    "        filters_dict[layer.name] = kept_filters\n",
    "        kept_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "def stich_filters(kept_filters, layer_name):\n",
    "    # By default, we will stich the best 64 (n*n) filters on a 8 x 8 grid.\n",
    "    n = int(np.sqrt(len(kept_filters)))\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top 64 filters.\n",
    "    kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    kept_filters = kept_filters[:n * n]\n",
    "\n",
    "    # build a black picture with enough space for\n",
    "    # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "    margin = 5\n",
    "    \n",
    "    width = n * img_width + (n - 1) * margin\n",
    "    height = n * img_height + (n - 1) * margin\n",
    "\n",
    "    stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "    # fill the picture with our saved filters\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, _ = kept_filters[i * n + j]\n",
    "            width_margin = (img_width + margin) * i\n",
    "            height_margin = (img_height + margin) * j\n",
    "            stitched_filters[\n",
    "                width_margin: width_margin + img_width,\n",
    "                height_margin: height_margin + img_height, :] = img[:, :, :3] \n",
    "\n",
    "    wandb.log({\"Filters\": wandb.Image(stitched_filters, caption=\"{}_{}\".format(model.name, layer_name)) })\n",
    "    # save the result to disk\n",
    "    save_img(\"reports/\" +directory + '/filters/{}_stitched_filters_{}.png'.format(model.name,\n",
    "             layer_name), stitched_filters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, kept_filters in filters_dict.items():\n",
    "    print('Stiching filters for {}'.format(layer_name))\n",
    "    stich_filters(kept_filters, layer_name)\n",
    "    print('number of filters kept:', len(kept_filters))\n",
    "    print('Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.cvae import CVAE\n",
    "#CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(0., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory + \"/activations\")] \n",
    "img_list = sorted(img_list, key=lambda x: int(x.split('_')[1]))\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    pdf.add_page()\n",
    "    pdf.image(\"reports/\" + directory + \"/activations/\"+img, y=50, w=200, h=200)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/report_activations.pdf\")\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(30., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory + \"/filters\")] \n",
    "img_list = sorted(img_list)\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    pdf.add_page()\n",
    "    pdf.image(\"reports/\" + directory + \"/filters/\"+img, y=75, w=150, h=150)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/report_filters.pdf\")\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(0., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory )] \n",
    "img_list = sorted(img_list)\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    if 'png' in img:\n",
    "        pdf.add_page()\n",
    "        pdf.image(\"reports/\" + directory + '/' +img, w=275, h=300)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/{}.pdf\".format(directory))\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
