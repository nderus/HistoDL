{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return np.array([i for i, elem in enumerate(lst) if condition(elem)])\n",
    "    \n",
    "def plot_2d_data_categorical(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  fig, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "  colors = np.array(['#7FFFD4', '#458B74', '#0000CD', '#EE3B3B', '#7AC5CD', '#66CD00',\n",
    "         '#EE7621', '#3D59AB', '#CD950C', '#483D8B'])\n",
    "  for i in range(len(data_2d)):\n",
    "      for k in range(category_count):\n",
    "\n",
    "        index = find_indices(y[i], lambda e: e == k)\n",
    "\n",
    "        data_2d_k = data_2d[i][index, ]\n",
    "        y_k = y[i][index]\n",
    "\n",
    "        if (titles != None):\n",
    "          axs[k,i].set_title(\"{} - Class: {}\".format(titles[i], k))\n",
    "\n",
    "        scatter = axs[k, i].scatter(data_2d_k[:, 0], data_2d_k[:, 1],\n",
    "                                s=1, c=colors[k], cmap=plt.cm.Paired)\n",
    "        axs[k, i].legend(*scatter.legend_elements())\n",
    "        axs[k, i].set_xlim([-3, 3])\n",
    "        axs[k, i].set_ylim([-3, 3])\n",
    "        wandb.log({\"Embdedding_classes\": wandb.Image(plt)})\n",
    "        fig.savefig('reports/' + directory + '/encoding_categorical')\n",
    "        \n",
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "    wandb.log({\"Embdedding\": wandb.Image(plt)})\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols,\n",
    "                          no_space_between_plots=False, figsize=(10, 10)):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_input(self, inputs, label_size=10):\n",
    "    image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                dtype ='float32')(inputs[0])\n",
    "    input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                    dtype ='float32')(inputs[1])\n",
    "    labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "    labels = tf.cast(labels, dtype='float32')\n",
    "    ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size])\n",
    "    labels = ones * labels\n",
    "    conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "    return  input_img, input_label, conditional_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean, z_log_var, input_label):\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                            mean=0., stddev=1.0, name='epsilon')\n",
    "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) \n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Val_Plot(loss, val_loss, reconstruction_loss, val_reconstruction_loss, kl_loss, val_kl_loss):\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize= (16,4))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(loss) + 1), loss)\n",
    "    ax1.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax1.set_title('History of Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "    ax2.plot(range(1, len(reconstruction_loss) + 1), reconstruction_loss)\n",
    "    ax2.plot(range(1, len(val_reconstruction_loss) + 1), val_reconstruction_loss)\n",
    "    ax2.set_title('History of reconstruction_loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('reconstruction_loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "    \n",
    "    ax3.plot(range(1, len(kl_loss) + 1), kl_loss)\n",
    "    ax3.plot(range(1, len(val_kl_loss) + 1), val_kl_loss)\n",
    "    ax3.set_title(' History of kl_loss')\n",
    "    ax3.set_xlabel(' Epochs ')\n",
    "    ax3.set_ylabel('kl_loss')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "    wandb.log({\"Training\": wandb.Image(plt)})\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from imageio import imread, imsave\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_dataset(name, root_folder):\n",
    "    data_folder = os.path.join(root_folder, name)\n",
    "    if name.lower() == 'celeba':\n",
    "        x = np.load(os.path.join(data_folder, 'train.npy'))\n",
    "        side_length = 64\n",
    "        channels = 3\n",
    "    else:\n",
    "        raise Exception('No such dataset called {}.'.format(name))\n",
    "    return x, side_length, channels\n",
    "\n",
    "\n",
    "def load_test_dataset(name, root_folder):\n",
    "    data_folder = os.path.join(root_folder, name)\n",
    "   \n",
    "    if name.lower() == 'celeba':\n",
    "        x = np.load(os.path.join(data_folder, 'test.npy'))\n",
    "        side_length = 64\n",
    "        channels = 3\n",
    "    else:\n",
    "        raise Exception('No such dataset called {}.'.format(name))\n",
    "    return x, side_length, channels\n",
    "\n",
    "\n",
    "def load_val_dataset(name, root_folder):\n",
    "    data_folder = os.path.join(root_folder, name)\n",
    "   \n",
    "    if name.lower() == 'celeba':\n",
    "        x = np.load(os.path.join(data_folder, 'val.npy'))\n",
    "        side_length = 64\n",
    "        channels = 3\n",
    "    else:\n",
    "        raise Exception('No such dataset called {}.'.format(name))\n",
    "    return x, side_length, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value:  0.0\n",
      "Max value:  1.0\n"
     ]
    }
   ],
   "source": [
    "train_x, _, _ = load_dataset(\"celeba\", \"datasets\")\n",
    "test_x, _, _ = load_test_dataset(\"celeba\", \"datasets\")\n",
    "val_x, _, _ = load_val_dataset(\"celeba\", \"datasets\")\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "val_x = val_x/255.0\n",
    "\n",
    "\n",
    "print('Min value: ',train_x.min())\n",
    "\n",
    "print('Max value: ',train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202599, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"datasets/list_attr_celeba.csv\"\n",
    "df = pd.read_csv(file_path, header = 0, index_col = 0).replace(-1,0)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162770, 40)\n",
      "(162770, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "train_y = np.array(df.iloc[ 0: 162770], dtype = \"float32\")\n",
    "print(train_y.shape)\n",
    "print(train_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(df.iloc[ 182637, 202599], dtype = \"float32\")\n",
    "print(test_y.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19867, 40)\n",
      "(19867, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "val_y = np.array(df.iloc[162770: 182637], dtype = \"float32\")\n",
    "print(val_y.shape)\n",
    "print(val_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x, dtype = \"float\")\n",
    "test_x = np.array(test_x, dtype = \"float\")\n",
    "val_x = np.array(val_x, dtype = \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVAE model**\n",
    "Creating a CVAE class and plugging encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu brings a lot of activation values = 0, leaky seems better\n",
    "# https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24\n",
    "\n",
    "def bn_relu(inputs):\n",
    "    bn = layers.BatchNormalization()(inputs)\n",
    "    relu = layers.LeakyReLU(0.2)(bn)\n",
    "    return(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder3( input_shape = (28, 28, 1),  label_size=10, encoded_dim = 2): \n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape[0],\n",
    "                input_shape[1], input_shape[2] + label_size), dtype='float32',\n",
    "                name='Input')\n",
    "    #inputs = layers.Input(shape = input_shape)\n",
    "    #labels_inputs = layers.Input(shape = (50, 50, 2))\n",
    "    #encoder_inputs = layers.Concatenate()([inputs, labels_inputs])\n",
    "    #block 1\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(inputs)\n",
    "    x = layers.Conv2D(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = bn_relu(x)\n",
    "    # block 2\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "\n",
    "    x = bn_relu(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2,name='S4')(x)\n",
    "\n",
    "    # block 3\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                padding='same',\n",
    "                name='block3_conv2')(x)    \n",
    "    x = bn_relu(x)            \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(encoded_dim * 2 )(x)\n",
    "    mu = layers.Dense(encoded_dim, name='mu')(y)\n",
    "    log_var = layers.Dense(encoded_dim, name='log_var')(y)\n",
    "    \n",
    "\n",
    "    model = keras.Model(inputs, [mu, log_var], name='encoder')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder3(input_shape, encoded_dim = 2,label_size=10): \n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(encoded_dim + label_size,),\n",
    "                                 name='decoder_input')\n",
    "    x = layers.Dense(encoded_dim)\n",
    "\n",
    "    x = layers.Dense(encoded_dim * 2 )\n",
    " \n",
    "    x = layers.Dense(input_shape[0]/2 * input_shape[1]/2 *64)(decoder_inputs)\n",
    "   \n",
    "    x = layers.Reshape(target_shape=(int(input_shape[0]/2),\n",
    "                     int(input_shape[1]/2), 64))(x)\n",
    "    x = bn_relu(x) \n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block4_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3),\n",
    "                    padding='same',\n",
    "                    name='up_block4_conv2')(x)  \n",
    "    x = bn_relu(x) \n",
    "    # block 2\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block5_conv2')(x)\n",
    "    x = bn_relu(x) \n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    # block 3\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                      padding='same',\n",
    "                      name='up_block6_conv1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(16, (3, 3),\n",
    "                    padding='same',\n",
    "                    name='up_block6_conv2')(x)\n",
    "    x = bn_relu(x)                                \n",
    "    outputs = layers.Conv2DTranspose(filters=input_shape[-1], kernel_size=2,\n",
    "                             strides=1, activation='sigmoid',padding='same')(x)\n",
    "\n",
    "    model = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 23:59:50.424712: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cvae_encoder = encoder3(encoded_dim = encoded_dim, input_shape = input_shape, label_size=40)\n",
    "cvae_decoder = decoder3(encoded_dim = encoded_dim, input_shape = input_shape, label_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 64, 64, 43)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 64, 64, 16)   6208        ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 64, 64, 16)   2320        ['block1_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 16)  64          ['block1_conv2[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 64, 64, 32)   4640        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 64, 64, 32)   9248        ['block2_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 32)  128         ['block2_conv2[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 32)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " S4 (MaxPooling2D)              (None, 32, 32, 32)   0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 32, 32, 64)   18496       ['S4[0][0]']                     \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['block3_conv2[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 65536)        0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          16777472    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          32896       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,921,552\n",
      "Trainable params: 16,921,328\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 168)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 65536)             11075584  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " up_block4_conv1 (Conv2DTran  (None, 32, 32, 64)       36928     \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block4_conv2 (Conv2DTran  (None, 32, 32, 64)       36928     \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " up_block5_conv1 (Conv2DTran  (None, 32, 32, 32)       18464     \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block5_conv2 (Conv2DTran  (None, 32, 32, 32)       9248      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 64, 64, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " up_block6_conv1 (Conv2DTran  (None, 64, 64, 16)       4624      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " up_block6_conv2 (Conv2DTran  (None, 64, 64, 16)       2320      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64, 64, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 64, 64, 3)        195       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,184,995\n",
      "Trainable params: 11,184,643\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, shape, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.shape = shape\n",
    "        self.latent_var = []\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.total_loss_no_weights_tracker = keras.metrics.Mean(name=\"loss_no_weights\")\n",
    "        #\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"val_reconstruction_loss\")\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "        self.val_total_loss_no_weights_tracker = keras.metrics.Mean(name=\"val_loss_no_weights\")\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.val_total_loss_tracker,\n",
    "            self.val_reconstruction_loss_tracker,\n",
    "            self.val_kl_loss_tracker,\n",
    "            self.total_loss_no_weights_tracker,\n",
    "            self.val_total_loss_no_weights_tracker,\n",
    "\n",
    "        ]\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        _, input_label, conditional_input = self.conditional_input(inputs)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        return self.decoder(z_cond)\n",
    "    \n",
    "    def conditional_input(self, inputs, label_size=10): \n",
    "  \n",
    "        image_size = [self.shape[0], self.shape[1], self.shape[2]]\n",
    "    \n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                      dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n",
    "\n",
    "    def sampling(self, z_mean, z_log_var, input_label):\n",
    "        if len(input_label.shape) == 1:\n",
    "            input_label = np.expand_dims(input_label, axis=0)\n",
    "\n",
    "        eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32,\n",
    "                               mean=0., stddev=1.0, name='epsilon')\n",
    "        z = z_mean + tf.exp(z_log_var / 2) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1)\n",
    "        return z_cond\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "            z_mean, z_log_var = self.encoder(conditional_input)\n",
    "            self.latent_var.append(z_log_var)\n",
    "            z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "            reconstruction = self.decoder(z_cond)\n",
    "            #reconstruction_loss = np.prod(self.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "            #                        tf.keras.backend.flatten(reconstruction))\n",
    "            reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img, # removed np.prod(self.shape) *\n",
    "                                    reconstruction), axis=(1, 2))            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                      - tf.exp(z_log_var))\n",
    "\n",
    "            kl_loss = tf.reduce_sum(kl_loss, axis=1) #sum over encoded dimensiosn, average over batch\n",
    "            total_loss_no_weights = reconstruction_loss + kl_loss\n",
    "            total_loss_no_weights = tf.reduce_mean(total_loss_no_weights)\n",
    "            kl_loss = self.beta * kl_loss\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            total_loss = tf.reduce_mean(total_loss) \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_no_weights_tracker.update_state(total_loss_no_weights)\n",
    "        #wandb.log({\"loss\": total_loss, \"reconstructon_loss\": reconstruction_loss, \"kl_loss\": kl_loss,})\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"loss_no_weights\": self.total_loss_no_weights_tracker.result()\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        \n",
    "        input_img, input_label, conditional_input = self.conditional_input(data)\n",
    "        z_mean, z_log_var = self.encoder(conditional_input)\n",
    "        z_cond = self.sampling(z_mean, z_log_var, input_label)\n",
    "        reconstruction = self.decoder(z_cond)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "                 keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))   \n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "                  - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=1)\n",
    "        total_loss_no_weights = reconstruction_loss + kl_loss\n",
    "        total_loss_no_weights = tf.reduce_mean(total_loss_no_weights)\n",
    "        kl_loss = self.beta * kl_loss\n",
    "        total_loss =  reconstruction_loss + kl_loss\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_no_weights_tracker.update_state(total_loss_no_weights)\n",
    "        return{\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "            \"loss_no_weights\": self.total_loss_no_weights_tracker.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl coefficient: 0.010\n",
      "Model: \"cvae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Functional)        [(None, 128),             16921552  \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 64, 64, 3)         11184995  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,106,559\n",
      "Trainable params: 28,105,971\n",
      "Non-trainable params: 588\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kl_coefficient = encoded_dim / (input_shape[0] * input_shape[1] * input_shape[2])\n",
    "print('kl coefficient: {:.3f}'.format(kl_coefficient))\n",
    "# from b vae paper, use beta = encoded_dimension / pixel_dimension i.e. -> 0.068\n",
    "cvae = CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)\n",
    "cvae.built = True\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_input = cvae.encoder.input[0]\n",
    "cvae_output = cvae.decoder.output\n",
    "mu = cvae.encoder.get_layer('mu').output\n",
    "log_var = cvae.encoder.get_layer('log_var').output\n",
    "\n",
    "learning_rate = 0.0001\n",
    "opt = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "cvae.compile(optimizer = opt)\n",
    "#cvae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean = np.random.normal(size=(100, 512))\n",
    "z_log_var =  np.random.normal(size=(100, 512))\n",
    "\n",
    "kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean)\n",
    "            - tf.exp(z_log_var))\n",
    "\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12288"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = batch_size\n",
    "#kl_loss = tf.reduce_sum(kl_loss, axis=1) #sum over encoded dimensiosn, average over batch\n",
    "#kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)) #sum over encoded dimensiosn, average over batch\n",
    "kl_loss.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=613545.0848890843>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = np.random.normal(size=(100, 32, 32, 3))\n",
    "reconstruction = np.random.normal(size=(100, 32, 32, 3))\n",
    "\n",
    "reconstruction_loss = np.prod(input_img.shape) * tf.keras.losses.MSE(tf.keras.backend.flatten(input_img),\n",
    "                                    tf.keras.backend.flatten(reconstruction)) \n",
    "\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim = batch_size\n",
    "reconstruction_loss = tf.reduce_sum(\n",
    "\n",
    "                keras.losses.MSE(input_img,\n",
    "                                    reconstruction), axis=(1, 2))\n",
    "\n",
    "reconstruction_loss.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "#wandb.init(project=\"my-test-project\", entity=\"nrderus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5\n",
    "\n",
    "train_x = train_x[:50000,:, :, :]\n",
    "train_y = train_y[:50000, :]\n",
    "\n",
    "\n",
    "val_x = val_x[:10000, :, :, :]\n",
    "val_y = val_y[:10000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5\n",
    "\n",
    "\n",
    "wandb.init(project=\"HistoDL\", entity=\"nrderus\",\n",
    "  config = {\n",
    "  \"dataset\": \"CelebA\",\n",
    "  \"model\": \"CVAE\",\n",
    "  \"encoded_dim\": encoded_dim,\n",
    "  \"kl_coefficient\": kl_coefficient,\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epoch_count,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"patience\": patience,\n",
    "  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "(<tf.Tensor 'IteratorGetNext:0' shape=(100, 64, 64, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(100, 40) dtype=float32>)\n",
      "(<tf.Tensor 'IteratorGetNext:0' shape=(100, 64, 64, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(100, 40) dtype=float32>)\n",
      "116/500 [=====>........................] - ETA: 4:55 - loss: 197.2409 - reconstruction_loss: 194.8278 - kl_loss: 2.4131"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb Cell 43'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=2'>3</a>\u001b[0m early_stop \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=3'>4</a>\u001b[0m              patience\u001b[39m=\u001b[39mpatience, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m cvae\u001b[39m.\u001b[39;49mfit([train_x, train_y],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=6'>7</a>\u001b[0m                    validation_data\u001b[39m=\u001b[39;49m([val_x, val_y],\u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=7'>8</a>\u001b[0m                    epochs\u001b[39m=\u001b[39;49mepoch_count,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=8'>9</a>\u001b[0m                    batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/HistoDL/toy3.ipynb#ch0000046vscode-remote?line=9'>10</a>\u001b[0m                    callbacks\u001b[39m=\u001b[39;49m[early_stop ])\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/wandb/integration/keras/keras.py:163\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    162\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "batch_size = 100\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "             patience=patience, restore_best_weights=True)\n",
    "\n",
    "history = cvae.fit([train_x, train_y],\n",
    "                   validation_data=([val_x, val_y],None),\n",
    "                   epochs=epoch_count,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[early_stop ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Val_Plot(history.history['loss'][1:],\n",
    "               history.history['val_loss'][1:],\n",
    "               history.history['reconstruction_loss'][1:],\n",
    "               history.history['val_reconstruction_loss'][1:],\n",
    "               history.history['kl_loss'][1:],\n",
    "               history.history['val_kl_loss'][1:]\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward the port 6006 on server on 12006 on  my machine\n",
    "# ssh -N -L 16006:127.0.0.1:6006 nicolas.derus2@137.204.48.211\n",
    "# access with http://127.0.0.1:16006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cvae.evaluate([test_x, test_y],None,\n",
    "            batch_size=batch_size,verbose=0)\n",
    "print('Test loss: {:.3f}'.format(test_loss[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embdedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_size = 10\n",
    "_, input_label_train, train_input = cvae.conditional_input([train_x, train_y])\n",
    "_, input_label_test, test_input = cvae.conditional_input([test_x, test_y])\n",
    "_, input_label_val, val_input = cvae.conditional_input([val_x, val_y])\n",
    "\n",
    "\n",
    "print(input_label_train.shape)\n",
    "print(train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean, train_log_var = cvae.encoder.predict(train_input)\n",
    "test_x_mean, test_log_var = cvae.encoder.predict(test_input)\n",
    "val_x_mean, val_log_var = cvae.encoder.predict(val_input)\n",
    "\n",
    "print(train_x_mean.shape)\n",
    "print(train_log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim > 2:\n",
    "    from sklearn import manifold\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    train_x_tsne = tsne.fit_transform(train_x_mean[:2000])\n",
    "    test_x_tsne = tsne.fit_transform(test_x_mean[:2000])\n",
    "    val_x_tsne = tsne.fit_transform(val_x_mean[:2000])\n",
    "    plot_2d_data( [train_x_tsne, test_x_tsne, val_x_tsne],\n",
    "            [train_y[:2000], test_y[:2000] ,val_y[:2000]],\n",
    "            ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "            [train_y, test_y ,val_y],\n",
    "            ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_dim == 2:\n",
    "    plot_2d_data( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(18,6))\n",
    "    plot_2d_data_categorical( [train_x_mean, test_x_mean, val_x_mean],\n",
    "                [train_y, test_y ,val_y],\n",
    "                ['Train','Test', 'Validation'],(12,36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructions...\n",
    "z_cond_train = sampling(train_x_mean, train_log_var, input_label_train)\n",
    "z_cond_test = sampling(test_x_mean, test_log_var, input_label_test)\n",
    "z_cond_val = sampling(val_x_mean, val_log_var, input_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_train = cvae.decoder(z_cond_train)\n",
    "reconstruction_test = cvae.decoder(z_cond_test)\n",
    "reconstruction_val = cvae.decoder(z_cond_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 10\n",
    "\n",
    "fig, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "  random_idx = random.randint(0, reconstruction_train.shape[0])\n",
    "  axs[0, i].imshow(train_x[random_idx])\n",
    "  axs[0, i].axis('off')\n",
    "  #axs[0, i].set_title(train_y[random_idx])\n",
    "  #axs[0, i].set_title( labels[int(train_y[random_idx])]  )\n",
    "  axs[1, i].imshow(reconstruction_train[random_idx])\n",
    "  axs[1, i].axis('off')\n",
    "  wandb.log({\"Reconstructions\": wandb.Image(plt)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrization(z_mean, z_log_var, input_label):\n",
    "    \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "    eps = tf.random.normal(shape = (input_label.shape[0], encoded_dim),\n",
    "                             mean = 0.0, stddev = 1.0)       \n",
    "    z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "    z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attr = 2\n",
    "_, axs = plt.subplots(2, image_count, figsize=(12, 3))\n",
    "for i in range(image_count):\n",
    "    attr_vect = np.zeros(40)\n",
    "    for attr in target_attr:\n",
    "        attr_vect[attr] = 1\n",
    "    labels = np.tile(attr_vect, reps = [batch_size, 1])\n",
    "    print(\"Generation ofimages with attributes: \", target_attr )\n",
    "    a = tf.convert_to_tensor(labels)\n",
    "    b = tf.concat([a, a], axis=0) # with 1 dimension, it fails...\n",
    "    z_cond = reparametrization(z_mean=0, z_log_var=0.3, input_label = b)\n",
    "    decoded_x = cvae_decoder.predict(z_cond)\n",
    "    digit_0 = decoded_x[0].reshape(input_shape) \n",
    "    digit_1 = decoded_x[1].reshape(input_shape) \n",
    "    axs[0, i].imshow(digit_0)\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(digit_1)\n",
    "    axs[1, i].axis('off')\n",
    "wandb.log({\"Generations\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model):\n",
    "    test = test_x[1]\n",
    "    plt.imshow(test)\n",
    "    #test = image.img_to_array(test)\n",
    "    test = np.expand_dims(test, axis=0)\n",
    "    test.shape\n",
    "    test_label = test_y[0]\n",
    "    img_tensor = [test, test_label]\n",
    "    from keras import models\n",
    "\n",
    "    # Extracts the outputs of the top 8 layers:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    layer_outputs = []\n",
    "    layer_names = []\n",
    "    for layer in model.layers[1:]:\n",
    "        \n",
    "        try: \n",
    "            layer_outputs.append(layer.get_output_at(1))\n",
    "            layer_names.append(layer.name)\n",
    "        \n",
    "        except:\n",
    "            layer_outputs.append(layer.output)\n",
    "            layer_names.append(layer.name)\n",
    "\n",
    "    # Creates a model that will return these outputs, given the model input:\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # This will return a list of 5 Numpy arrays:\n",
    "    # one array per layer activation\n",
    "    if 'encoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        activations = activation_model.predict(conditional_input) #for encoder\n",
    "\n",
    "    if 'decoder' in model.name:\n",
    "        input_img, input_label, conditional_input = cvae.conditional_input(img_tensor)\n",
    "        input_label = np.expand_dims(input_label, axis=0)\n",
    "        z_mean, z_log_var = cvae.encoder(conditional_input)\n",
    "        z_cond = cvae.sampling(z_mean, z_log_var, input_label)\n",
    "        \n",
    "        activations = activation_model.predict(z_cond) #for decoder\n",
    "    \n",
    "    for activation, name in zip(activations[0:], layer_names[0:]):\n",
    "        print(name)\n",
    "        print(activation.shape)\n",
    "    \n",
    "    for counter, (activation, name) in enumerate(zip(activations[0:], layer_names[0:])):\n",
    "        print(name)\n",
    "        plot_filters(activation, name, counter, model_name=model.name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_filters(activation_layer, layer_name, counter, model_name):\n",
    "    if len(activation_layer.shape) == 2: # if flat layer\n",
    "        print('flat')\n",
    "        return None\n",
    "        if activation_layer.shape[1] == 1875:\n",
    "            activation_layer = activation_layer.reshape(1, 25, 25, 3)\n",
    "        if activation_layer.shape[1] == 1024:\n",
    "           activation_layer = activation_layer.reshape(1, 16, 16, 4)\n",
    "        if activation_layer.shape[1] == 512:\n",
    "           activation_layer = activation_layer.reshape(1, 8, 8, 8)\n",
    "\n",
    "    n = math.floor(np.sqrt(activation_layer.shape[3]))\n",
    "\n",
    "    if int(n + 0.5) ** 2 == activation_layer.shape[3]:\n",
    "\n",
    "        m = n\n",
    "    else:\n",
    "        m = math.floor(activation_layer.shape[3] / n)\n",
    "\n",
    "    if activation_layer.shape[3] == 1:\n",
    "        fig, ax = plt.subplots(1, 1, sharex='col', sharey='row',\n",
    "                                figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "\n",
    "        ax.imshow(activation_layer[0,:, :, 0], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "        fig.savefig(\"reports/\" + directory + '/activations/{}_{}_activations_{}.png'.format(model_name,\n",
    "                     counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "            \n",
    "    if n == 1:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, sharex='col', sharey='row',figsize=(15, 15))\n",
    "        fig.suptitle(layer_name)\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(activation_layer[0,:, :, i], cmap='viridis')\n",
    "        wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "        fig.savefig(\"reports/\" +directory +'/activations/{}_{}_activations_{}.png'.format(model_name, counter, layer_name))\n",
    "        return None   \n",
    "\n",
    "    fig, ax = plt.subplots(n, m, sharex='col', sharey='row',figsize=(15, 15))\n",
    "    fig.suptitle(layer_name)\n",
    "    \n",
    " \n",
    "\n",
    "    filter_counter = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(activation_layer[0,:, :, filter_counter], cmap='viridis')\n",
    "            filter_counter += 1\n",
    "            if filter_counter == (activation_layer.shape[3] ):\n",
    "                break\n",
    "\n",
    "    wandb.log({\"Activations\": wandb.Image(plt, caption=\"{}_{}\".format(model_name, layer_name)) })\n",
    "    fig.savefig(\"reports/\" + directory + \"/activations/{}_{}_activations_{}.png\".format(model_name, counter, layer_name))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_activations( cvae.encoder)\n",
    "visualize_activations(cvae.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cvae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    #img = img[ 25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conditional_input( inputs, label_size=10): \n",
    "  \n",
    "        image_size = [input_shape[0], input_shape[1], input_shape[2]]\n",
    "\n",
    "        input_img = layers.InputLayer(input_shape=image_size,\n",
    "                                        dtype ='float32')(inputs[0])\n",
    "        input_label = layers.InputLayer(input_shape=(label_size, ),\n",
    "                                        dtype ='float32')(inputs[1])\n",
    "\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, label_size])\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        ones = tf.ones([inputs[0].shape[0]] + image_size[0:-1] + [label_size]) \n",
    "        labels = ones * labels\n",
    "        conditional_input = layers.Concatenate(axis=3)([input_img, labels]) \n",
    "        return  input_img, input_label, conditional_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nth_filter_loss(filter_index, layer_name):\n",
    "    \"\"\"\n",
    "    We build a loss function that maximizes the activation\n",
    "    of the nth filter of the layer considered\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a connection between the input and the target layer\n",
    "    \n",
    "    submodel = tf.keras.models.Model([model.inputs[0]],\n",
    "                                     [model.get_layer(layer_name).output])\n",
    "\n",
    "# Initiate random noise\n",
    "\n",
    "    input_img_data = np.random.random((1, input_shape[0], input_shape[1],\n",
    "                                     input_shape[2]))\n",
    "\n",
    "    input_img_data =(input_img_data - 0.5) * 0.25\n",
    "    # Cast random noise from np.float64 to tf.float32 Variable\n",
    "    input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "\n",
    "    data = [input_img_data, train_y[0]]\n",
    "    _, _, conditional_input_img = filter_conditional_input(data) \n",
    "    conditional_input_img= tf.Variable(tf.cast(conditional_input_img,\n",
    "                                         tf.float32))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = submodel(conditional_input_img)\n",
    "            loss_value = tf.reduce_mean(outputs[:, 2:-2, 2:-2, filter_index]) #removed borders in loss\n",
    "        grads = tape.gradient(loss_value, conditional_input_img)\n",
    "        normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads)))\n",
    "                                   + 1e-5)\n",
    "        #normalized_grads = tf.math.l2_normalize(grads)\n",
    "        conditional_input_img.assign_add(normalized_grads * step_size)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    #iterate = K.function([input_img], [loss_value, grads])\n",
    "\n",
    "    if loss_value > 0:\n",
    "        \n",
    "        #img = conditional_input_img.numpy().astype(np.float64)\n",
    "        #img = img.squeeze()\n",
    "        #img = deprocess_image(img) / 255.\n",
    "        img = conditional_input_img.numpy().astype(np.float64)\n",
    "        img = img.squeeze()\n",
    "        img = deprocess_image(img)\n",
    "        kept_filters.append((img, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the generated pictures for each filter.\n",
    "img_width = input_shape[0]\n",
    "img_height = input_shape[1]\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "print(input_img.shape)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "#layer_dict = dict([(layer.name, layer) for layer in model.layers[0:]])\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_filters = [layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "step_size = 10.\n",
    "kept_filters = []\n",
    "filters_dict = dict()\n",
    "for layer_name in layers_filters:\n",
    "    if 'conv' in layer_name:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        print('Processing filter for layer:', layer_name)\n",
    "        for filter_index in range(min(layer.output.shape[-1], 100)):\n",
    "            # print('Processing filter %d' % filter_index)\n",
    "            build_nth_filter_loss(filter_index, layer_name)\n",
    "        filters_dict[layer.name] = kept_filters\n",
    "        kept_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "def stich_filters(kept_filters, layer_name):\n",
    "    # By default, we will stich the best 64 (n*n) filters on a 8 x 8 grid.\n",
    "    n = int(np.sqrt(len(kept_filters)))\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top 64 filters.\n",
    "    kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    kept_filters = kept_filters[:n * n]\n",
    "\n",
    "    # build a black picture with enough space for\n",
    "    # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "    margin = 5\n",
    "    \n",
    "    width = n * img_width + (n - 1) * margin\n",
    "    height = n * img_height + (n - 1) * margin\n",
    "\n",
    "    stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "    # fill the picture with our saved filters\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, _ = kept_filters[i * n + j]\n",
    "            width_margin = (img_width + margin) * i\n",
    "            height_margin = (img_height + margin) * j\n",
    "            stitched_filters[\n",
    "                width_margin: width_margin + img_width,\n",
    "                height_margin: height_margin + img_height, :] = img[:, :, :3] \n",
    "\n",
    "    wandb.log({\"Filters\": wandb.Image(stitched_filters, caption=\"{}_{}\".format(model.name, layer_name)) })\n",
    "    # save the result to disk\n",
    "    save_img(\"reports/\" +directory + '/filters/{}_stitched_filters_{}.png'.format(model.name,\n",
    "             layer_name), stitched_filters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, kept_filters in filters_dict.items():\n",
    "    print('Stiching filters for {}'.format(layer_name))\n",
    "    stich_filters(kept_filters, layer_name)\n",
    "    print('number of filters kept:', len(kept_filters))\n",
    "    print('Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.cvae import CVAE\n",
    "#CVAE(cvae_encoder, cvae_decoder, kl_coefficient, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(0., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory + \"/activations\")] \n",
    "img_list = sorted(img_list, key=lambda x: int(x.split('_')[1]))\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    pdf.add_page()\n",
    "    pdf.image(\"reports/\" + directory + \"/activations/\"+img, y=50, w=200, h=200)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/report_activations.pdf\")\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(30., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory + \"/filters\")] \n",
    "img_list = sorted(img_list)\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    pdf.add_page()\n",
    "    pdf.image(\"reports/\" + directory + \"/filters/\"+img, y=75, w=150, h=150)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/report_filters.pdf\")\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a instance of fpdf\n",
    "pdf = FPDF(format='A4')\n",
    "pdf.set_margins(0., 0.,  0.)\n",
    "pdf.set_auto_page_break(0)\n",
    "# use a folder that you created (here it's imgs)\n",
    "img_list = [x for x in os.listdir(\"reports/\" + directory )] \n",
    "img_list = sorted(img_list)\n",
    "# add new pages with the image \n",
    "for img in img_list:\n",
    "    if 'png' in img:\n",
    "        pdf.add_page()\n",
    "        pdf.image(\"reports/\" + directory + '/' +img, w=275, h=300)\n",
    "# save the output file\n",
    "pdf.output(\"reports/\" + directory +\"/{}.pdf\".format(directory))\n",
    "print(\"Adding all your images into a pdf file\")\n",
    "print(\"Images pdf is created and saved it into the following path folder:\\n\",\n",
    "      os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
